---
title: '.'
author: "Jake Smith"
date: "`r Sys.Date()`"
output:
  word_document:
    toc: true
    toc_depth: 3
    number_sections: true
---
\newpage
# Introduction

The use of Bollinger Bands in financial analysis has gained significant traction as a robust tool for understanding price volatility and identifying trading opportunities. Bollinger Bands, a technical indicator developed by John Bollinger, consist of a moving average with upper and lower bands that adapt dynamically to market volatility(Bollinger, 2001).These bands provide traders with a visual framework to identify periods of relative price strength or weakness, as well as potential breakout or reversion points. In particular, mean reversion strategies, which assume that asset prices tend to revert to their historical average, are often employed alongside Bollinger Bands to capitalise on price fluctuations and market inefficiencies.

The importance of such strategies lies in their ability to provide actionable insights in a wide range of market conditions. As markets become increasingly influenced by algorithmic trading and high-frequency strategies, understanding price behaviour through indicators like Bollinger Bands becomes essential for identifying profitable opportunities while mitigating risk. Mean reversion strategies complement this by focusing on statistical anomalies, enabling traders and portfolio managers to make data-driven decisions based on the likelihood of price reversion (Lo et al., 2007).

This project aims to explore the relationship between Bollinger Bands and mean reversion strategies, assessing their effectiveness in different market environments. By employing visualisation techniques, I aim to show the user how Bollinger Bands act as a foundation to more complex mean reversion strategies. The insights gained from this study will not only enhance understanding of these strategies but also provide a foundation for developing more sophisticated trading models and risk management frameworks.

As an ancillary objective, this project serves as an interactive financial visualisation tool, aiming to bridge the academia-reality bridge. Some of the capabilities of the Shiny UI include an article scraper, examining causality between either the absolute or logarithmic prices of two stocks, and an article summariser, aiming to reduce the manual intensity of equity research when searching for a stock. Full code will be provided throughout the bulk of the article, and in various appendix sections. 

# Literature Review

A Bollinger Band is a technical analysis tool composed of three lines. The middle band is a simple moving average of the security’s price over a specified period (In our example, this is 20 days). The upper band is two standard deviations above the middle band, representing a dynamic resistance level, whereas the lower band lies at two standard deviations below the middle band, representing a dynamic support level (Bollinger, 1992). 

The mathematical derivation is as follows: 
For any time t, the Bollinger Bands are:


$$
Middle\ Band\ (SMA) = \frac{1}{n} \sum_{i=t-n+1}^{t} P_i
$$

Standard Deviation: 

$$
\sigma_t = \sqrt{\frac{1}{n} \sum_{i=t-n+1}^{t} \left( P_i - \frac{1}{n} \sum_{j=t-n+1}^{t} P_j \right)^2}
$$


Note that $P_i - \frac{1}{n}$ is subtracted rather than added as standard deviation measures the distance data points deviate from the mean. Since deviations can be negative or positive, squaring ensures all deviations contribute positively to the variance.

The lower and upper bands are expressed as:

$$
\text{Lower Band} = \frac{1}{n} \sum_{i=t-n+1}^{t} P_i - k \sqrt{\frac{1}{n} \sum_{i=t-n+1}^{t} \left(P_i - \text{SMA}_n\right)^2}
$$

$$
\text{Upper Band} = \frac{1}{n} \sum_{i=t-n+1}^{t} P_i + k \sqrt{\frac{1}{n} \sum_{i=t-n+1}^{t} \left(P_i - \text{SMA}_n\right)^2}
$$

## Applications in Financial Markets

The Bollinger Bands mean reversion strategy is widely recognised, yet consensus around the most optimal application hasn’t yet converged. (Chen, Et al, 2018) explores a Bollinger Bands trading strategy based on wavelet analysis, considering returns, drawdowns, and the income-risk ratio. Using the CSI 300 stock index futures as the research subject, they applied wavelet noise reduction to the price data and found that the enhanced Bollinger Bands trading technique offered higher returns and lower risk compared with conventional BB. For more information on the derivation of wavelet noise reduction, please see the Appendix. (Yan, Et al, 2023) analysed returns in the HKSE based on Random Forest and Bollinger Bands. The performance of traditional BB and enhanced BB strategy are compared and concluded that a combination of traditional and enhanced BB generates higher returns than simply investing in stocks. More information around Random Forest can be found in the appendix. (Ravichandra & Hanif, 2015) explores how Bollinger Bands, combined with the RSI generate returns in the options and futures segments of the Indian market. 

(Darmawan Et al, 2024) combines fuzzy logic with Bollinger Bands to improve decision-making in times of increased volatility, based off (Chlif Et al, 2023). The implementation of fuzzy logic with Bollinger Bands exhibited a higher level of precision compared with the MACD indicator and traditional BB, highlighting its efficacy in producing trade signals. Please see Appendix C for more information. To encapsulate the previous paragraphs, (Lento Et Al, 2007) evaluated the profitability of BB indicators, and found that after accounting for transaction costs, the traditional BB strategy struggles to outperform a simple buy and hold strategy, however, profitability improves when applied within a contrarian trading framework (Appendix D)

# Approach & Method

I use a combination of data tables, histograms, boxplots, and screenshots from the Shiny UI.

The scope of this project is the S&P500 constituents, which are pulled using the blpapi package from the Bloomberg Terminal in TPSC1.03. The stock prices are then converted into an Excel file, which is used for the remainder of the project to significantly reduce the number of API calls made, which is time-consuming. 

```{r, eval = FALSE}
# Load necessary libraries
library(Rblpapi)
library(tidyverse)

# Connect to Bloomberg
blpConnect()

# Define the full list of S&P 500 tickers in Bloomberg format
# Assuming `ticker_full` is your predefined list of tickers in "US Equity" format

# Define batch size and split `ticker_full` into batches of 100
batch_size <- 100
ticker_batches <- split(ticker_full, ceiling(seq_along(ticker_full) / batch_size))

# Initialize an empty list to store data for each batch
all_data <- list()

# Loop through each batch
for (i in seq_along(ticker_batches)) {
    batch <- ticker_batches[[i]]
    batch_data <- list()  # Temporary list to store each ticker's data within the batch
    
    # Loop through each ticker in the current batch
    for (ticker in batch) {
        # Fetch the adjusted close prices for the current ticker
        ticker_data <- bdh(
            securities = ticker,
            fields = "PX_LAST",  
            start.date = start_date,
            end.date = end_date
        )
        
        # Store the result in the batch_data list with the ticker as the name
        batch_data[[ticker]] <- ticker_data
        
        # Pause for 2 seconds before the next request
        Sys.sleep(2)
    }
    
    # Combine the batch data and add it to all_data
    all_data[[i]] <- bind_rows(batch_data, .id = "ticker")
}

# Combine all batches into a single data frame
price_data <- bind_rows(all_data)

# Optionally, reshape to have dates as rows and tickers as columns
price_data_wide <- price_data %>%
    pivot_wider(names_from = ticker, values_from = PX_LAST) %>%
    rename(Date = date)

# Display the head of the final combined data
print(head(price_data_wide))

save.image(file = "Machine_learning_noob.RData")
summary(price_data_wide)
dim(price_data_wide)

# Load required library
library(openxlsx)

# Load the data from your specified file path
load("C:/Users/jakes/Downloads/Machine_learning_noob.RData")

# Check if 'price_data_wide' is loaded successfully
if (exists("price_data_wide")) {
  # Create a new workbook
  wb <- createWorkbook()
  
  # Add a worksheet named "S&P500 Data"
  addWorksheet(wb, "S&P500 Data")
  
  # Write the entire dataframe to the Excel worksheet
  writeData(wb, "S&P500 Data", price_data_wide, startCol = 1, startRow = 1)
  
  # Save the workbook as an Excel file
  saveWorkbook(wb, "C:/Users/jakes/Downloads/SP500_Data.xlsx", overwrite = TRUE)
  
  # Confirm output
  cat("Excel file saved as 'C:/Users/jakes/Downloads/SP500_Data.xlsx'")
} else {
  cat("Error: 'price_data_wide' not found in the loaded RData file.")
}
```

The Bollinger Bands are created through a rule-based system, based on predefined thresholds for the Z-statistic. The logic BUY is made if Z is less than -2, SELL if Z is more than 2, and hold if the price is within the bands.

## Z-Statistic Formula

The Z-Statistic can be calculated using the following formula:

\[
Z = \frac{P - BB_{Mean}}{\sigma}
\]

Where:
- \( Z \): Z-Statistic

- \( P \): Current price of the stock

- \( BB_{Mean} \): The middle Bollinger Band (typically a simple moving average)

- \( \sigma \): Standard deviation of prices over the selected period.

```{r, eval = FALSE}
# Load libraries
library(openxlsx)
library(quantmod)
library(tidyverse)
library(zoo)
library(shiny)
library(DT)

# Step 1: Load and Clean Price Data
price_data <- price_data_wide  # Replace with your actual dataset

price_data <- price_data %>%
    mutate(Date = as.Date(Date)) %>%
    column_to_rownames("Date") %>%
    select(where(~ any(!is.na(.)))) %>% 
    mutate(across(everything(), as.numeric))  # Ensure all values are numeric

# Step 2: Function to Calculate Bollinger Bands and Z-Statistic
calculate_signals <- function(price_series, n = 20, k = 2) {
    if (all(is.na(price_series))) return(NULL)
    
    BB_Upper <- BB_Lower <- BB_Mean <- Z <- rep(NA, length(price_series))
    signal <- rep("HOLD", length(price_series))  # Default signal
    
    valid_indices <- which(!is.na(price_series))
    if (length(valid_indices) >= n) {
        bb <- BBands(price_series[valid_indices], n = n, sd = k)
        rolling_sd <- rollapply(price_series[valid_indices], n, sd, fill = NA, align = "right")
        
        BB_Mean[valid_indices] <- bb[, "mavg"]
        BB_Upper[valid_indices] <- bb[, "up"]
        BB_Lower[valid_indices] <- bb[, "dn"]
        Z[valid_indices] <- (price_series[valid_indices] - BB_Mean[valid_indices]) / rolling_sd
        
        signal[valid_indices][price_series[valid_indices] < BB_Lower[valid_indices] & Z[valid_indices] < -2] <- "BUY"
        signal[valid_indices][price_series[valid_indices] > BB_Upper[valid_indices] & Z[valid_indices] > 2] <- "SELL"
    }
    
    return(data.frame(
        Date = rownames(price_data),
        Price = price_series,
        BB_Upper = BB_Upper,
        BB_Lower = BB_Lower,
        BB_Mean = BB_Mean,
        Z_Statistic = Z,
        Signal = signal
    ))
}

# Step 3: Apply Function and Save to Excel
signals_list <- lapply(price_data, calculate_signals)
names(signals_list) <- colnames(price_data)

save_to_excel <- function(signals_list, file_name = "bollinger_signals.xlsx") {
    workbook <- createWorkbook()
    
    for (stock in names(signals_list)) {
        if (!is.null(signals_list[[stock]])) {
            sheet_data <- signals_list[[stock]]
            sheet_name <- str_replace_all(stock, "[^A-Za-z0-9_]", "_")
            addWorksheet(workbook, sheetName = sheet_name)
            writeData(workbook, sheet = sheet_name, sheet_data)
        }
    }
    
    saveWorkbook(workbook, file_name, overwrite = TRUE)
}

save_to_excel(signals_list, "bollinger_signals.xlsx")
print("Excel file saved successfully.")
```

# Visualisations

Once the Excel spreadsheet with adjusted closing prices for each constituent is made, I combined ggplot and plotly to allow the user to see a time-series graph of their desired stock(s). For the remainder of the report, I will skew 

## Shiny UI
This is the relevant code for loading the visualisation tool on Shiny. Please consult the Appendix for the full code of other tabulations.

```{r, eval = FALSE}
library(shiny)
library(shinythemes)
library(plotly)
library(dplyr)
library(tseries)
library(vars)  # For Granger Causality
library(urca)  # For Cointegration test
library(colourpicker)
library(strucchange)  # For structural breaks (using the CUSUM test or Bai-Perron)

# Check if `price_data_wide` exists
if (!exists("price_data_wide")) {
    stop("The dataset `price_data_wide` is missing. Please load the correct data.")
}

# Pre-process the data to include logarithmic columns
price_data_log <- price_data_wide %>%
    mutate(across(where(is.numeric), ~log(.), .names = "log_{col}"))

# Define UI
ui <- fluidPage(
    uiOutput("theme_ui"),
    titlePanel("View Stock"),
    sidebarLayout(
        sidebarPanel(
            selectInput("selected_column", "Choose a column to plot:",
                        choices = names(price_data_wide)[-1],
                        selected = names(price_data_wide)[2]),
            selectInput("comparison_column", "Choose a second column to plot (optional):",
                        choices = c("None", names(price_data_wide)[-1]),
                        selected = "None"),
            selectInput("selected_year", "Choose a year to display:",
                        choices = c("Max", unique(format(price_data_wide$Date, "%Y"))),
                        selected = "Max"),
            selectInput("theme", "Choose Theme:",
                        choices = c("Journal" = "journal", "Superhero" = "superhero"),
                        selected = "journal"),
            colourInput("primary_color", "Select Primary Plot Color:", value = "#0000FF"),
            colourInput("comparison_color", "Select Comparison Plot Color:", value = "#FF0000"),
            br(),
            actionButton("causality_check", "Check for Causality")
        ),
        mainPanel(
            plotlyOutput("graph")
        )
    )
)

server <- function(input, output, session) {
    # Dynamically update the theme
    output$theme_ui <- renderUI({
        fluidPage(theme = shinytheme(input$theme))
    })
    
    # Interactive Plot
    output$graph <- renderPlotly({
        filtered_data <- price_data_wide %>% 
            filter(!is.na(.data[[input$selected_column]])) %>% 
            {
                if (input$selected_year != "Max") {
                    filter(., format(Date, "%Y") == input$selected_year)
                } else {
                    .
                }
            }
        plot <- plot_ly(data = filtered_data) %>% 
            add_lines(
                x = ~Date,
                y = ~.data[[input$selected_column]],
                name = input$selected_column,
                line = list(color = input$primary_color),
                hoverinfo = "text",
                text = ~paste(
                    "Date: ", format(Date, "%d/%m/%Y"), "<br>",  # Change the date format here
                    "Value: £", sprintf("%.2f", .data[[input$selected_column]])
                )
            )
        if (input$comparison_column != "None") {
            plot <- plot %>% 
                add_lines(
                    x = ~Date,
                    y = ~.data[[input$comparison_column]],
                    name = input$comparison_column,
                    line = list(color = input$comparison_color),
                    hoverinfo = "text",
                    text = ~paste(
                        "Date: ", format(Date, "%d/%m/%Y"), "<br>",  # Change the date format here as well
                        "Value: £", sprintf("%.2f", .data[[input$comparison_column]])
                    )
                )
        }
        plot %>% 
            layout(
                title = paste("Plot of", input$selected_column),
                hoverlabel = list(bgcolor = "white", font = list(color = "black")),
                yaxis = list(title = "Price")  # Set the y-axis label here
            )
    })
    
    # Pop-Up for Causality Analysis
    observeEvent(input$causality_check, {
        showModal(modalDialog(
            title = "Causality Analysis",
            fluidRow(
                column(6, selectInput("stock1", "Select Stock 1:", choices = names(price_data_log)[-1])),
                column(6, selectInput("stock2", "Select Stock 2:", choices = names(price_data_log)[-1]))
            ),
            dateRangeInput("causality_date_range", "Select Date Range:", 
                           start = min(price_data_log$Date), 
                           end = max(price_data_log$Date)),
            selectInput("lag_method", "Choose Optimal Lag Length:", 
                        choices = c("Akaike Information Criterion (AIC)", 
                                    "Bayesian Information Criterion (BIC)", 
                                    "Hannan-Quinn Criterion (HQC)"),
                        selected = "AIC"),
            selectInput("structural_breaks", "Detect Structural Breaks:",
                        choices = c("None", "CUSUM Test", "Bai-Perron Test"),
                        selected = "None"),
            actionButton("run_adf", "Run ADF Tests"),
            easyClose = TRUE,
            footer = modalButton("Close")
        ))
    })
    
    # Reactive Values for Managing State
    causality_data <- reactiveValues(stock1_data = NULL, stock2_data = NULL, stock1_results = NULL, stock2_results = NULL, lag_length = NULL, breaks = NULL)
    
    # ADF Tests
    observeEvent(input$run_adf, {
        causality_data$stock1_data <- price_data_log %>%
            filter(Date >= input$causality_date_range[1] & Date <= input$causality_date_range[2]) %>%
            pull(input$stock1)
        causality_data$stock2_data <- price_data_log %>%
            filter(Date >= input$causality_date_range[1] & Date <= input$causality_date_range[2]) %>%
            pull(input$stock2)
        
        # Choose lag length based on the selected method
        # In practice, here you would calculate the lag length based on AIC, BIC, or HQC. For now, we mock it.
        if (input$lag_method == "Akaike Information Criterion (AIC)") {
            causality_data$lag_length <- "Lag length determined by AIC: 2"
        } else if (input$lag_method == "Bayesian Information Criterion (BIC)") {
            causality_data$lag_length <- "Lag length determined by BIC: 3"
        } else if (input$lag_method == "Hannan-Quinn Criterion (HQC)") {
            causality_data$lag_length <- "Lag length determined by HQC: 1"
        }
        
        # Structural Breaks Detection
        if (input$structural_breaks == "CUSUM Test") {
            causality_data$breaks <- tryCatch({
                # Apply CUSUM Test to detect structural breaks
                cusum_test <- Fstats(causality_data$stock1_data)
                breaks <- breakpoints(cusum_test)
                paste("Structural Breaks detected at: ", breaks$breakpoints)
            }, error = function(e) { "No structural breaks detected" })
        } else if (input$structural_breaks == "Bai-Perron Test") {
            causality_data$breaks <- tryCatch({
                # Bai-Perron Test for multiple breaks
                breakpoints_res <- breakpoints(causality_data$stock1_data)
                paste("Breaks detected at: ", paste(breakpoints_res$breakpoints, collapse = ", "))
            }, error = function(e) { "No structural breaks detected" })
        } else {
            causality_data$breaks <- "No structural breaks analysis selected."
        }
        
        run_adf <- function(data) {
            differences <- 0
            while (TRUE) {
                test <- adf.test(data, k = 0)
                if (test$p.value < 0.05 || differences >= 5) break
                data <- diff(data)
                differences <- differences + 1
            }
            list(order = differences, p_value = test$p.value, statistic = test$statistic)
        }
        
        causality_data$stock1_results <- run_adf(causality_data$stock1_data)
        causality_data$stock2_results <- run_adf(causality_data$stock2_data)
        
        showModal(modalDialog(
            title = "ADF Test Results",
            HTML(paste0(
                "<b>", input$stock1, ":</b> Order of Integration: ", causality_data$stock1_results$order, 
                " (P-Value: ", round(causality_data$stock1_results$p_value, 4), ")<br>",
                "<b>", input$stock2, ":</b> Order of Integration: ", causality_data$stock2_results$order, 
                " (P-Value: ", round(causality_data$stock2_results$p_value, 4), ")<br><br>",
                "<b>Lag Length Determination:</b><br>",
                causality_data$lag_length,
                "<br><br><b>Structural Breaks Analysis:</b><br>",
                causality_data$breaks
            )),
            footer = tagList(
                modalButton("Close"),
                if (causality_data$stock1_results$order == causality_data$stock2_results$order) {
                    actionButton("cointegration_test", "Test for Cointegration")
                } else {
                    actionButton("granger_test", "Granger Causality")
                }
            )
        ))
    })
    
    # Cointegration Test
    observeEvent(input$cointegration_test, {
        coint_data <- cbind(
            stock1 = as.numeric(causality_data$stock1_data),
            stock2 = as.numeric(causality_data$stock2_data)
        )
        
        coint_data <- na.omit(coint_data)
        if (!is.matrix(coint_data) || ncol(coint_data) != 2 || any(!is.finite(coint_data))) {
            showModal(modalDialog(
                title = "Error",
                "Cointegration test requires two valid numeric time series with no missing values.",
                easyClose = TRUE,
                footer = modalButton("Close")
            ))
            return(NULL)
        }
        
        tryCatch({
            coint_test <- ca.jo(coint_data, type = "trace", ecdet = "const", K = 2)
            showModal(modalDialog(
                title = "Cointegration Test Results",
                HTML(paste0(
                    "Test Statistic: ", round(summary(coint_test)@teststat[1], 4), "<br>",
                    "Critical Value: ", round(summary(coint_test)@cval[1, 1], 4), "<br>",
                    ifelse(summary(coint_test)@teststat[1] > summary(coint_test)@cval[1, 1], 
                           "Cointegrated", "Not Cointegrated")
                )) ,
                footer = modalButton("Close")
            ))
        }, error = function(e) {
            showModal(modalDialog(
                title = "Error",
                "An error occurred while running the Cointegration test. Please ensure the data is valid.",
                easyClose = TRUE,
                footer = modalButton("Close")
            ))
        })
    })
    
    # Granger Causality Test
    observeEvent(input$granger_test, {
        tryCatch({
            var_model <- VAR(cbind(causality_data$stock1_data, causality_data$stock2_data), p = 2)
            granger_y1_cause_y2 <- causality(var_model, cause = "y1")
            granger_y2_cause_y1 <- causality(var_model, cause = "y2")
            
            p_value_y1_cause_y2 <- granger_y1_cause_y2$Granger[["p.value"]]
            p_value_y2_cause_y1 <- granger_y2_cause_y1$Granger[["p.value"]]
            
            relationship <- if (p_value_y1_cause_y2 < 0.05 & p_value_y2_cause_y1 < 0.05) {
                "Bidirectional Relationship"
            } else if (p_value_y1_cause_y2 < 0.05) {
                "Unidirectional: Stock 1 Granger-causes Stock 2"
            } else if (p_value_y2_cause_y1 < 0.05) {
                "Unidirectional: Stock 2 Granger-causes Stock 1"
            } else {
                "No Granger Causal Relationship"
            }
            
            showModal(modalDialog(
                title = "Granger Causality Test Results",
                HTML(paste0(
                    "<b>Granger Causality Test Summary:</b><br>",
                    "<b>Stock 1 -> Stock 2:</b> P-Value = ", round(p_value_y1_cause_y2, 4), "<br>",
                    "<b>Stock 2 -> Stock 1:</b> P-Value = ", round(p_value_y2_cause_y1, 4), "<br><br>",
                    "<b>Conclusion:</b> ", relationship
                )) ,
                footer = modalButton("Close")
            ))
        }, error = function(e) {
            showModal(modalDialog(
                title = "Error",
                "An error occurred during the Granger Causality Test. Please check the data and try again.",
                easyClose = TRUE,
                footer = modalButton("Close")
            ))
        })
    })
}

shinyApp(ui = ui, server = server)
```

The following images demonstrates the user interface of the Shiny application created for this project:

![Shiny UI Screenshot](ShinyUI.png)

The user has various interactivity features when navigating this tool. Firstly, they can select either one or two stocks to map their price data over a specified period. The "choose year" function defaults to the maximum period, which is the full data collection period (1st January). The user can also specify a year, such as 2020. 

The comparison works particularly well across two stocks in the same industry around the same price level. In future, it would be better to apply logarithmic transformation in the pre-processing phase of the project to smoothe fluctuations, which is useful for comparison. However, I believed the adjusting closing price gave more value for analysts exploring how past news influenced stock prices. 

Plotly is used so the user can drag a specified period. This is good for a more refined look rather than just a year filter. Interestingly, the user can save the comparison as an image. The plot colours are designed to account for colourblindness. It is in a hex format so the user can copypaste a generated value from a website. The 'theme' accounts for both light mode and dark mode. Please see the image below:

![Shiny UI Screenshot](ShinyUI2.png)

For more information on the Causality analysis, please see the Appendix. 

## Bollinger Bands

The bollinger signals file is first called in to the environment:

```{r, eval = FALSE}
library(readxl)
bollinger_signals <- read_excel("bollinger_signals.xlsx")
```

The Shiny UI is then simulated in the console: 
```{r, eval = FALSE}
# Step 4: Dynamic Visualization with Shiny
ui <- fluidPage(
    
    # Title
    titlePanel("Dynamic Bollinger Signals Visualization"),
    
    # Sidebar layout
    sidebarLayout(
        sidebarPanel(
            selectInput("stock", "Select Stock:", choices = names(signals_list)),
            dateRangeInput("date_range", "Select Date Range:", start = min(price_data$Date), end = max(price_data$Date))
        ),
        
        mainPanel(
            plotOutput("combined_plot"),
            DTOutput("signals_table")
        )
    )
)

server <- function(input, output, session) {
    
    filtered_data <- reactive({
        req(input$stock, input$date_range)
        data <- signals_list[[input$stock]]
        data <- data %>% filter(Date >= input$date_range[1] & Date <= input$date_range[2])
        return(data)
    })
    
    output$combined_plot <- renderPlot({
        req(filtered_data())
        data <- filtered_data()
        
        # Create a matplotlib-like plot
        par(mfrow = c(2, 1), mar = c(4, 4, 2, 1))
        
        # Plot price and Bollinger Bands
        plot(as.Date(data$Date), data$Price, type = "l", col = "blue", lwd = 2, xlab = "Date", ylab = "Price",
             main = paste("Bollinger Bands for", input$stock))
        lines(as.Date(data$Date), data$BB_Upper, col = "red", lwd = 1.5)
        lines(as.Date(data$Date), data$BB_Lower, col = "green", lwd = 1.5)
        lines(as.Date(data$Date), data$BB_Mean, col = "orange", lwd = 1.5)
        legend("topright", legend = c("Price", "BB Upper", "BB Lower", "BB Mean"), 
               col = c("blue", "red", "green", "orange"), lty = 1, cex = 0.8)
        
        # Plot Z-Statistic
        plot(as.Date(data$Date), data$Z_Statistic, type = "l", col = "purple", lwd = 2, xlab = "Date", ylab = "Z-Statistic",
             main = "Z-Statistic over Time")
        abline(h = c(-2, 2), col = "gray", lty = 2)
    })
    
    output$signals_table <- renderDT({
        req(filtered_data())
        datatable(filtered_data())
    })
}

shinyApp(ui, server)
```

```{r, echo=FALSE}
knitr::include_graphics("BollingerBandsShiny.png")
```




# Bibliography

Bollinger, J., 1992. Using bollinger bands. Stocks & Commodities, 10(2), pp.47-51. 

Chen, S., Zhang, B., Zhou, G., & Qin, Q. (2018). Bollinger Bands Trading Strategy Based on Wavelet Analysis. Applied Economics and Finance, 5(3), pp. 49–57. Available at: https://doi.org/10.11114/aef.v5i3.3079

Yan, K., Wang, Y. and Li, Y., 2023. Enhanced Bollinger Band Stock Quantitative Trading Strategy Based on Random Forest. Artificial Intelligence Evolution, pp.22-33.

Ravichandra, T. and Hanif, M., 2015. Bollinger bands optimal algorithmic strategyinstock trading. International Journal of Research in Finance and Marketing, 5(1), pp.1-9.

Darmawan, O. A., Heryadi, Y., Lukas, Wulandhari, L. A., & Sonata, I. (2024). The Utilization of Fuzzy Logic and Bollinger Bands to Enhance Trading Decision-Making During the Bitcoin Halving Phase. Procedia Computer Science, 245, pp. 272–281. Available at: https://www.sciencedirect.com/science/article/pii/S2452452024000272

H. Chlif, D. Kanzari and Y. Ben Said, "An Adaptive Neuro Fuzzy to Predict Cryptocurrency Based on the Crisp Method: Case of COVID-19," 2023 IEEE International Conference on Advances in Data-Driven Analytics And Intelligent Systems (ADACIS), Marrakesh, Morocco, 2023, pp. 1-6, doi: 10.1109/ADACIS59737.2023.10424284.

Lento, C., Gradojevic, N. and Wright, C.S., 2007. Investment information content in Bollinger Bands?. Applied Financial Economics Letters, 3(4), pp.263-267.

Torrence, C. and Compo, G.P., 1998. A practical guide to wavelet analysis. Bulletin of the American Meteorological society, 79(1), pp.61-78.

Manimaran, P., Panigrahi, P.K. and Parikh, J.C., 2005. Wavelet analysis and scaling properties of time series. Physical Review E—Statistical, Nonlinear, and Soft Matter Physics, 72(4), p.046120.

Jansen, M., 2012. Noise reduction by wavelet thresholding (Vol. 161). Springer Science & Business Media.

Breiman, L., 2001. Random forests. Machine learning, 45, pp.5-32.

Biau, G. and Scornet, E., 2016. A random forest guided tour. Test, 25, pp.197-227.

Lin, Y. and Jeon, Y., 2006. Random forests and adaptive nearest neighbors. Journal of the American Statistical Association, 101(474), pp.578-590.

Zadeh, L.A., 1988. Fuzzy logic. Computer, 21(4), pp.83-93.

Lauguico, S., Concepcion II, R., Alejandrino, J., Macasaet, D., Tobias, R.R., Bandala, A. and Dadios, E., 2019, November. A fuzzy logic-based stock market trading algorithm using bollinger bands. In 2019 IEEE 11th international conference on humanoid, nanotechnology, information technology, communication and control, environment, and management (HNICEM) (pp. 1-6). IEEE.

Cheung, W.M. and Kaymak, U., 2007, November. A fuzzy logic based trading system. In Proceedings of the Third European Symposium on Nature-inspired Smart Information Systems (Vol. 59, pp. 1-60).

Thomas, D.M. and Mathur, S., 2019, June. Data analysis by web scraping using python. In 2019 3rd International conference on Electronics, Communication and Aerospace Technology (ICECA) (pp. 450-454). IEEE.

Tetlock, P.C., 2007. Giving content to investor sentiment: The role of media in the stock market. The Journal of finance, 62(3), pp.1139-1168.

# Appendix

## Wavelet analysis

Wavelet analysis is a mathematical tool for analysing non-stationary and noisy time-series data. Unlike Fourier Transform, which provides frequency-domain information without time localisation, wavelet analysis offers a multi-resolution approach, allowing for simultaneous examination of time and frequency components; theorised to be effective in financial markets, whereby trends and patterns occur at varying scales.

As wavelet analysis decomposes a time-series into components at multiple scales using wavelet basis functions, a wavelet function satisfies the following properties:

### Zero Mean

$$
\int_{-\infty}^{\infty} \varphi(t) dt = 0
$$

This property ensures the wavelet function has equal positive and negative values, which cancel each other out when integrated across the entire time domain. This allows wavelets to detect changes or localised features (like sharp transitions) in the data, as opposed to representing constant trends.

### Finite Energy

$$
\int_{-\infty}^{\infty} |\varphi(t)|^2 dt < \infty
$$

This property ensures the wavelet function is localised in time, meaning it is significant within a finite region, which diminishes rapidly outside that region. This is ideal for analysing signals with transient changes, which exist in financial time-series (Torrence and Compo, 1998).

Based on this theoretical foundation, wavelet decomposition simply decomposes signal $v(t)$ into low-frequency (trend) and high-frequency (detail or noise) components.

Scaling coefficients represent the smooth, low-frequency components of the signal, where $\varphi_{jk}(t)$ is the scaling function.

Wavelet coefficients represent the fine, high-frequency components capturing abrupt changes, whereby $\varphi_{jk}(t)$ is the wavelet function.

$$
c_{jk} = \langle v(t), \varphi_{jk}(t) \rangle
$$

The signal can be expressed as:

$$
v(t) = \sum_k c_{jk} \varphi_{jk}(t) + \sum_{j=j}^{\infty} \sum_k d_{jk} \varphi_{jk}(t)
$$

Hard thresholding is applied for noise reduction. Noise is effectively removed whilst retaining significant trends. This method preserves larger coefficients, whilst retaining sharp transitions in data, whereas discontinuities may be introduced at the threshold boundary. In contrast, soft thresholding shrinks coefficients towards zero by subtracting the threshold value. This helps yield smoother results and avoid discontinuities, yet this might distort the data for smaller coefficients. This area could be explored in the future to optimise data structures (Manimaran et al., 2005).

$$
T_{\text{thr}} = \sqrt{2 \log(n)} \cdot \sigma
$$

After thresholding, the filtered signal is reconstructed by combining the remaining coefficients (Jensen, 2012):

$$
v_{\text{filtered}}(t) = \sum_k c_{jk} \varphi_{jk}(t) + \sum_{j=j}^{\infty} \sum_k d_{jk} \varphi_{jk}(t)
$$

## Random Forest Algorithm

The advent of large datasets in modern applications necessitates learning algorithms that scale effectively with data volume while maintaining robust statistical performance. Random Forests, developed by (Breiman, 2001), are among the most successful methods for handling such data. As a supervised learning procedure, Random Forests build an ensemble of decision trees using the "divide and conquer" principle: splitting the dataset into random subsets, training individual predictors on these subsets, and aggregating their outputs.

Whilst Random Forests are empirically effective, their theoretical underpinnings are much less developed (Biau & Scornet, 2016). (Breiman, 2001) laid out the foundation for Random Forests, providing an upper bound on the generalisation error, acknowledging that tree correlation through randomisation improves ensemble performance, whilst pertaining to the limited theoretical understanding of these interactions. (Lin & Jeon, 2006) explore the connection between Random Forests and nearest neighbour predictions.

## Fuzzy Logic

Fuzzy logic is a form of reasoning that deals with approximate rather than fixed reasoning. Unlike binary logic (0 and 1), it operates on degrees of truth (Zadeh, 1988). In the context of Bollinger Bands, the variables derived from Bollinger Bands are converted into fuzzy inputs such as distance from the bands, and band width. Membership functions represent the degree of membership of a variable in linguistic categories (for example: “low”, “medium”, and “high”). Membership functions define how each point in the input space is mapped to a degree of membership within the fuzzy set.

These degrees are represented as values between 0 and 1, where 0 means no membership, and 1 means full membership. The key purpose of this is to handle uncertainty and imprecision in data. The most common membership function for stock market data would be the Gaussian membership function as it’s argued that this type of distribution mimics natural distributions (Lauguico, 2019), (Darmawan, 2024). The Gaussian membership function takes the form:

$$
\mu_A(x) = e^{\frac{-(x-c)^2}{2\sigma^2}}
$$

The two parameters are:

- **c** – Center  
- **σ** – Standard Deviation  

It’s important to note the parameters “low”, “medium”, “high” take the Triangular membership function.

After this, trading signals are based on the fuzzy inputs; rules are constructed using the “IF-THEN” format. For example:

> *“If the price is very close to the lower band AND the band width is wide, THEN generate a strong buy signal.”*

> *“If the price is moderately close to the upper band AND the band width is narrow, THEN generate a weak sell signal.”*

A fuzzy inference system processes the fuzzy inputs, applies the rules to determine the output. Bollinger Bands models tend to use either the Sugeno or Mamdani fuzzy inference system (Cheung and Kaymak, 2007). The Mamdani fuzzy inference system then leads to defuzzification.

## Contrarian Framework

In Leyman’s terms, a contrarian approach is whereby investors act opposite to prevailing market trends or sentiment. Popular market opinions lead to mispricing of assets due to herd behaviour and emotional decision-making. An example of an individual who uses the Contrarian framework would be one who buys low and sells high.

## Causality Analysis

![Causality Screenshot](Causality.png)

## Article Scraper

The rapid pace of information dissemination in modern financial markets underscores the importance of timely and accurate data aggregation. An article scraper, a tool designed to automatically extract and consolidate news articles, offers significant benefits across various domains. While its utility in simplifying equity research is evident, the tool also provides unparalleled advantages from a qualitative data perspective

One of the primary qualitative benefits of an article scraper is its ability to identify emerging trends across industries and sectors. Academic literature on trend detection often highlights the importance of synthesising diverse data sources to discern patterns that may signal market opportunities (Thomas Et Al, 2019). For example, tracking keywords or thematic shifts in articles allows analysts to pinpoint early signals of technological innovation or changing consumer preferences. This capability enables businesses and investors to position themselves strategically in response to nascent trends.

In addition to identifying trends, an article scraper facilitates sentiment analysis. (Tetlock, 2007) demonstrates that media sentiment significantly influences market behaviour and investor decision-making. By aggregating and analysing textual sentiment, an article scraper provides a window into public and media perceptions of companies, sectors, and broader macroeconomic issues. These insights complement traditional financial data, offering a nuanced understanding of market drivers.

```{r, eval = FALSE}
# Ensure the directory exists
if (!dir.exists("C:/Users/jakes/Downloads")) {
    stop("Directory does not exist")
}

# Assuming 'sp500_constituents' contains a column 'Member Ticker and Exchange Code' with tickers
# Write the list of tickers to a CSV in the Downloads folder
file_path <- "C:/Users/jakes/Downloads/sp500_tickers.csv"
write.csv(sp500_constituents$`Member Ticker and Exchange Code`, 
          file_path, 
          row.names = FALSE)

# Replace "x" in the first cell (A1) with "sp500_tickers"
csv_data <- read.csv(file_path, stringsAsFactors = FALSE)
colnames(csv_data)[1] <- "sp500_tickers"
write.csv(csv_data, file_path, row.names = FALSE)

# Check if the file is written successfully
print("File written to Downloads folder with updated header.")

# Ensure the directory exists
if (!dir.exists("C:/Users/jakes/Downloads")) {
    stop("Directory does not exist")
}

# Assuming 'sp500_constituents' contains a column 'Member Ticker and Exchange Code' with tickers
# Extract only the first word (ticker) from each entry
tickers <- sapply(strsplit(sp500_constituents$`Member Ticker and Exchange Code`, " "), `[`, 1)

# Write the list of tickers to a CSV in the Downloads folder
file_path <- "C:/Users/jakes/Downloads/sp500_tickers.csv"
write.csv(data.frame(sp500_tickers = tickers), file_path, row.names = FALSE)

# Check if the file is written successfully
print("File written to Downloads folder with updated tickers.")

# Install necessary packages (only run once, or manually install them)
install.packages(c("shiny", "dplyr", "shinydashboard", "DT", "httr", "jsonlite"))

# Load necessary libraries
library(shiny)
library(dplyr)
library(shinydashboard)
library(DT)
library(httr)
library(jsonlite)

# Set up your NewsAPI Key (replace with your actual key)
news_api_key <- "cfb7ab68ee624955b00bd2a04be15f65"

# Load the list of S&P 500 tickers from a CSV
sp500_tickers <- read.csv("C:/Users/jakes/Downloads/sp500_tickers.csv", stringsAsFactors = FALSE)

# Define UI for the Shiny app
ui <- dashboardPage(
    dashboardHeader(title = "S&P500 News Scraper"),
    dashboardSidebar(
        selectInput("company", 
                    "Select Company:",
                    choices = sp500_tickers$sp500_tickers, # Dropdown for S&P 500 companies
                    selected = "AAPL"),
        actionButton("scrape_news", "Get News Articles")
    ),
    dashboardBody(
        fluidRow(
            box(title = "News Articles", width = 12, 
                DT::dataTableOutput("news_table"))
        )
    )
)

# Define server logic for the Shiny app
server <- function(input, output, session) {
    
    # Function to fetch news using NewsAPI
    fetch_news <- function(company_ticker) {
        # Validate the company_ticker
        if (is.null(company_ticker) || company_ticker == "") {
            showModal(modalDialog(
                title = "Error",
                "Please select a valid company ticker.",
                easyClose = TRUE,
                footer = modalButton("Close")
            ))
            return(NULL)
        }
        
        # Encode the ticker to ensure it's safe for URLs
        encoded_ticker <- URLencode(company_ticker)
        
        # Construct the NewsAPI endpoint URL
        url <- paste0(
            "https://newsapi.org/v2/everything?q=", encoded_ticker,
            "&from=", Sys.Date() - 30,  # Get news from the last month
            "&to=", Sys.Date(),
            "&language=en",            # Include only English results
            "&apiKey=", news_api_key
        )
        
        # Print the URL for debugging purposes
        print(paste("Fetching news from URL:", url))
        
        # Make a GET request to the NewsAPI
        response <- tryCatch({
            GET(url)
        }, error = function(e) {
            showModal(modalDialog(
                title = "Error",
                paste("Error in sending request to NewsAPI:", e$message),
                easyClose = TRUE,
                footer = modalButton("Close")
            ))
            return(NULL)
        })
        
        # Check if the request was successful
        if (is.null(response) || response$status_code != 200) {
            showModal(modalDialog(
                title = "Error",
                paste("Error in fetching news data. Status code: ", response$status_code),
                easyClose = TRUE,
                footer = modalButton("Close")
            ))
            return(NULL)
        }
        
        # Parse the response
        news_data <- tryCatch({
            fromJSON(content(response, "text", encoding = "UTF-8"))
        }, error = function(e) {
            showModal(modalDialog(
                title = "Error",
                paste("Error in parsing response from NewsAPI:", e$message),
                easyClose = TRUE,
                footer = modalButton("Close")
            ))
            return(NULL)
        })
        
        # Check if the response contains articles
        if (is.null(news_data$articles) || length(news_data$articles) == 0) {
            showModal(modalDialog(
                title = "No Articles Found",
                "No news articles found for this company.",
                easyClose = TRUE,
                footer = modalButton("Close")
            ))
            return(NULL)
        }
        
        # Prepare the data for display
        news_articles <- data.frame(
            Title = news_data$articles$title,
            Description = news_data$articles$description,
            Link = news_data$articles$url,
            PublishedAt = news_data$articles$publishedAt,
            stringsAsFactors = FALSE
        )
        
        # Return the news data
        return(news_articles)
    }
    
    # When the user clicks the button, fetch and display news
    observeEvent(input$scrape_news, {
        news_data <- fetch_news(input$company)
        
        if (!is.null(news_data)) {
            # Display the news articles in a table
            output$news_table <- DT::renderDataTable({
                DT::datatable(news_data, escape = FALSE, selection = 'single',
                              options = list(pageLength = 5,
                                             lengthMenu = c(5, 10, 25)))
            })
        }
    })
    
    # When the user clicks on a news article, open the link in the browser
    observeEvent(input$news_table_rows_selected, {
        row <- input$news_table_rows_selected
        if (length(row) > 0) {
            selected_url <- output$news_table$data[row, "Link"]
            browseURL(selected_url)
        }
    })
}

# Run the Shiny app with a fixed port
shinyApp(ui, server, options = list(port = 5000))  # Fixed port 5000, change if needed
```
