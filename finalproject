---
title: "Determining the Efficacy of Bollinger Bands: AAPL Case Study"
author: "100357048"
date: "`r Sys.Date()`"
output:
  pdf_document:
    latex_engine: xelatex
    highlight: pygments
    keep_tex: true
fontsize: 11pt
geometry: a4paper, margin=1in
header-includes:
  - \usepackage{amsmath}
  - \usepackage{graphicx}
  - \setkeys{Gin}{width=\linewidth, keepaspectratio}
  - \usepackage{listings}
  - \lstset{
      breaklines=true,
      breakatwhitespace=true,
      basicstyle=\ttfamily\small,
      numbers=left,
      numberstyle=\tiny,
      frame=single,
      tabsize=4
    }
  - \usepackage{fvextra}
  - \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
  - \usepackage{float}
---

\newpage
# Introduction

Over the past 30 years, Bollinger Bands have emerged as a widely used tool in financial analysis, offering a robust framework for understanding price volatility and identifying trading opportunities. Developed by (Bollinger,2001), this technical indicator comprises a moving average flanked by dynamically adjusting upper and lower bands, providing a visual mechanism to detect price strength, weakness, and potential breakouts or reversions. Mean reversion strategies, which leverage the tendency of asset prices to revert to their historical averages, are frequently employed alongside Bollinger Bands to exploit price fluctuations and market inefficiencies.

The relevance of such strategies lies in their adaptability across diverse market conditions. In the context of increasingly algorithmic and high-frequency trading, Bollinger Bands offer a critical lens for interpreting price behavior and uncovering profitable opportunities, while mean reversion strategies enhance decision-making by capitalizing on statistical anomalies (Lo et al., 2007).

This project evaluates the efficacy of Bollinger Bands as a standalone indicator, assessing their performance across varying market scenarios and demonstrating their foundational role in more advanced mean reversion strategies. Additionally, the project includes interactive Shiny-based financial visualisation tools that bridges the gap between academic research and practical application. Features include article scraping, causal analysis between stock price movements, correlation heatmaps, and automated article summarisation to streamline equity research. Full code and detailed explanations are provided throughout the appendix. 

# Literature Review

A Bollinger Band is a technical analysis tool composed of three lines. The middle band is a simple moving average of the security’s price over a specified period (In our example, this is 20 days). The upper band is two standard deviations above the middle band, representing a dynamic resistance level, whereas the lower band lies at two standard deviations below the middle band, representing a dynamic support level (Bollinger, 1992). 

The mathematical derivation is as follows: 
For any time t, the Bollinger Bands are:


$$
Middle\ Band\ (SMA) = \frac{1}{n} \sum_{i=t-n+1}^{t} P_i
$$

Standard Deviation: 

$$
\sigma_t = \sqrt{\frac{1}{n} \sum_{i=t-n+1}^{t} \left( P_i - \frac{1}{n} \sum_{j=t-n+1}^{t} P_j \right)^2}
$$


Note that $P_i - \frac{1}{n}$ is subtracted rather than added as standard deviation measures the distance data points deviate from the mean. Since deviations can be negative or positive, squaring ensures all deviations contribute positively to the variance.

The lower and upper bands are expressed as:

$$
\text{Lower Band} = \frac{1}{n} \sum_{i=t-n+1}^{t} P_i - k \sqrt{\frac{1}{n} \sum_{i=t-n+1}^{t} \left(P_i - \text{SMA}_n\right)^2}
$$

$$
\text{Upper Band} = \frac{1}{n} \sum_{i=t-n+1}^{t} P_i + k \sqrt{\frac{1}{n} \sum_{i=t-n+1}^{t} \left(P_i - \text{SMA}_n\right)^2}
$$

## Applications in Financial Markets

The Bollinger Bands mean reversion strategy is widely recognised, but consensus on its optimal application remains elusive. (Chen et al,2018) explored a Bollinger Bands strategy enhanced with wavelet noise reduction, using the CSI 300 stock index futures, and demonstrated higher returns and lower risk compared to conventional approaches (Appendix A). Similarly, (Yan et al,2023) combined Random Forest with Bollinger Bands in the HKSE, concluding that integrating traditional and enhanced strategies outperforms simple stock investments (Appendix B). (Ravichandra and Hanif, 2015) examined Bollinger Bands alongside the RSI in the Indian options and futures markets, highlighting their synergy.

More recently, (Darmawan et al, 2024), building on (Chlif et al, 2023), integrated fuzzy logic with Bollinger Bands to improve decision-making during periods of heightened volatility. Their results showed greater precision in generating trade signals compared to traditional BB or MACD indicators (Appendix C). Finally, (Lento et al, 2007) assessed the profitability of Bollinger Bands and found that, after accounting for transaction costs, traditional strategies struggle to outperform a buy-and-hold approach. However, profitability improves when used within a contrarian trading framework (Appendix D).

# Approach & Method

Historical price data for S&P 500 stocks was retrieved using the Bloomberg Terminal API. Since Bloomberg imposes API call limits, I implemented a batch-processing system to optimise retrieval. The data was then reshaped into a wide format, making it suitable for downstream analyses and visualisations.

```{r, eval = FALSE}
# Define batch size and split tickers into batches
batch_size <- 100
ticker_batches <- split(ticker_full, ceiling(seq_along(ticker_full) / batch_size))

# Fetch data for each batch
all_data <- lapply(ticker_batches, function(batch) {
    lapply(batch, function(ticker) {
        bdh(securities = ticker, fields = "PX_LAST", start_date = start_date, end_date = end_date)
    })
})
```

Once the raw data was collected, it was transformed into a wide format, where each column represents the price series of a specific stock, and rows correspond to dates. This transformation allows for efficient analysis across multiple stocks simultaneously.

```{r, eval = FALSE}
# Reshape data into wide format
price_data_wide <- pivot_wider(names_from = ticker, values_from = PX_LAST) %>%
    rename(Date = date)
```

This converts the raw data into a structured dataset suitable for time-series analysis and visualisation.

The final step was exporting the transformed dataset to an Excel file for ease of use and reproducibility, which also allowed for visual inspection and validation of the data before further analysis.

```{r, eval = FALSE}
write.xlsx(price_data_wide, "S&P500_Data.xlsx", overwrite = TRUE)
```

## Z-Statistic Formula

The Z-Statistic can be calculated using the following formula:

\[
Z = \frac{P - BB_{Mean}}{\sigma}
\]

Where:
- \( Z \): Z-Statistic

- \( P \): Current price of the stock

- \( BB_{Mean} \): The middle Bollinger Band (typically a simple moving average)

- \( \sigma \): Standard deviation of prices over the selected period.

```{r, eval = FALSE}
# Load libraries
library(openxlsx)
library(quantmod)
library(tidyverse)
library(zoo)
library(shiny)
library(DT)

# Step 1: Load and Clean Price Data
price_data <- price_data_wide  # Replace with your actual dataset

price_data <- price_data %>%
    mutate(Date = as.Date(Date)) %>%
    column_to_rownames("Date") %>%
    select(where(~ any(!is.na(.)))) %>% 
    mutate(across(everything(), as.numeric))  # Ensure all values are numeric

# Step 2: Function to Calculate Bollinger Bands and Z-Statistic
calculate_signals <- function(price_series, n = 20, k = 2) {
    if (all(is.na(price_series))) return(NULL)
    
    BB_Upper <- BB_Lower <- BB_Mean <- Z <- rep(NA, length(price_series))
    signal <- rep("HOLD", length(price_series))  # Default signal
    
    valid_indices <- which(!is.na(price_series))
    if (length(valid_indices) >= n) {
        bb <- BBands(price_series[valid_indices], n = n, sd = k)
        rolling_sd <- rollapply(price_series[valid_indices], n, sd, fill = NA, align = "right")
        
        BB_Mean[valid_indices] <- bb[, "mavg"]
        BB_Upper[valid_indices] <- bb[, "up"]
        BB_Lower[valid_indices] <- bb[, "dn"]
        Z[valid_indices] <- (price_series[valid_indices] - BB_Mean[valid_indices]) / rolling_sd
        
        signal[valid_indices][price_series[valid_indices] < BB_Lower[valid_indices] & Z[valid_indices] < -2] <- "BUY"
        signal[valid_indices][price_series[valid_indices] > BB_Upper[valid_indices] & Z[valid_indices] > 2] <- "SELL"
    }
    
    return(data.frame(
        Date = rownames(price_data),
        Price = price_series,
        BB_Upper = BB_Upper,
        BB_Lower = BB_Lower,
        BB_Mean = BB_Mean,
        Z_Statistic = Z,
        Signal = signal
    ))
}

# Step 3: Apply Function and Save to Excel
signals_list <- lapply(price_data, calculate_signals)
names(signals_list) <- colnames(price_data)

save_to_excel <- function(signals_list, file_name = "bollinger_signals.xlsx") {
    workbook <- createWorkbook()
    
    for (stock in names(signals_list)) {
        if (!is.null(signals_list[[stock]])) {
            sheet_data <- signals_list[[stock]]
            sheet_name <- str_replace_all(stock, "[^A-Za-z0-9_]", "_")
            addWorksheet(workbook, sheetName = sheet_name)
            writeData(workbook, sheet = sheet_name, sheet_data)
        }
    }
    
    saveWorkbook(workbook, file_name, overwrite = TRUE)
}

save_to_excel(signals_list, "bollinger_signals.xlsx")
print("Excel file saved successfully.")
```

# Visualisations

After generating the Excel spreadsheet with adjusted closing prices for all constituents, I integrated ggplot with plotly to enable interactive time-series visualisation for selected stocks.

## Shiny UI

This visualisation displays the adjusted closing prices of all 503 S&P constituents from January 2010 to October 31, 2024, the dataset's storage date. The main body focuses on the key components driving this visualisation, with the full code available in the Appendix.

### Interactive plotting with plotly


```{r, eval = FALSE}
output$graph <- renderPlotly({
    filtered_data <- price_data_wide %>% 
        filter(!is.na(.data[[input$selected_column]])) %>% 
        {
            if (input$selected_year != "Max") {
                filter(., format(Date, "%Y") == input$selected_year)
            } else {
                .
            }
        }
    plot <- plot_ly(data = filtered_data) %>% 
        add_lines(
            x = ~Date,
            y = ~.data[[input$selected_column]],
            name = input$selected_column,
            line = list(color = input$primary_color),
            hoverinfo = "text",
            text = ~paste(
                "Date: ", format(Date, "%d/%m/%Y"), "<br>",  
                "Value: £", sprintf("%.2f", .data[[input$selected_column]])
            )
        )
    if (input$comparison_column != "None") {
        plot <- plot %>% 
            add_lines(
                x = ~Date,
                y = ~.data[[input$comparison_column]],
                name = input$comparison_column,
                line = list(color = input$comparison_color),
                hoverinfo = "text",
                text = ~paste(
                    "Date: ", format(Date, "%d/%m/%Y"), "<br>",  
                    "Value: £", sprintf("%.2f", .data[[input$comparison_column]])
                )
            )
    }
    plot %>% 
        layout(
            title = paste("Plot of", input$selected_column),
            hoverlabel = list(bgcolor = "white", font = list(color = "black")),
            yaxis = list(title = "Price")
        )
})
```
![Shiny UI Screenshot](ShinyUI.png)

Plotly’s dynamic exploration feature enables users to select specific S&P 500 constituents, tailoring the dataset to their interests. The comparison functionality overlays multiple data series on a single chart, facilitating the identification of relationships between variables. Hover functionality provides detailed insights (e.g., date, value) for each data point, eliminating the need to consult raw data tables. Dynamic year selection allows users to focus on specific time periods, highlighting temporal patterns or events. Additionally, colour selectors for primary and comparison columns enhance user control over chart aesthetics. 

### User Interface design
```{r, eval = FALSE}
ui <- fluidPage(
    uiOutput("theme_ui"),
    titlePanel("View Stock"),
    sidebarLayout(
        sidebarPanel(
            selectInput("selected_column", "Choose a column to plot:",
                        choices = names(price_data_wide)[-1],
                        selected = names(price_data_wide)[2]),
            selectInput("comparison_column", "Choose a second column to plot (optional):",
                        choices = c("None", names(price_data_wide)[-1]),
                        selected = "None"),
            selectInput("selected_year", "Choose a year to display:",
                        choices = c("Max", unique(format(price_data_wide$Date, "%Y"))),
                        selected = "Max"),
            selectInput("theme", "Choose Theme:",
                        choices = c("Journal" = "journal", "Superhero" = "superhero"),
                        selected = "journal"),
            colourInput("primary_color", "Select Primary Plot Color:", value = "#0000FF"),
            colourInput("comparison_color", "Select Comparison Plot Color:", value = "#FF0000"),
            br(),
            actionButton("causality_check", "Check for Causality")
        ),
        mainPanel(
            plotlyOutput("graph")
        )
    )
)
```
The structured layout, with a sidebar for inputs and a main panel for outputs, ensures a logical workflow that minimises cognitive load and directs user focus to the visualised data. The responsive design enabled by fluidPage ensures seamless interaction across devices and screen sizes, enhancing the application's versatility.

### Theme Selection
```{r, eval = FALSE}
output$theme_ui <- renderUI({
    fluidPage(theme = shinytheme(input$theme))
})
```
![Shiny UI Screenshot](ShinyUI2.png)

### Dynamic filtering of data
```{r, eval = FALSE}
filtered_data <- price_data_wide %>% 
    filter(!is.na(.data[[input$selected_column]])) %>% 
    {
        if (input$selected_year != "Max") {
            filter(., format(Date, "%Y") == input$selected_year)
        } else {
            .
        }
    }
```

The code dynamically filters out rows with missing values (NA) in the selected column, ensuring only valid data is visualised. This reduces noise, minimises errors, and enhances responsiveness by limiting the amount of processed data based on user input.

### Comments

AAPL exhibits a long-term bullish trend, reflective of its consistent revenue growth (5 year average YoY growth rate of 9% as of 2024, increases to 12% in 2022.) and market leadership in technology (please see Appendix for the scripts I ran to obtain the Excel files to compute the growth rates), leading to elevated valuations determining continued upwards price action. The COVID-19 period saw an accelerated price growth during this period, which is linked to an increased resilience on technology, with heightened demand for AAPL products. Supressed interest rates and global stimulus measures further helped valuations for AAPL.

With a beta above 1, this graph highlights AAPL’s higher volatility compared to other S&P 500 constituents, underscoring its sensitivity to market sentiment and macroeconomic trends. For instance, the notable correction in 2022 aligned with the Federal Reserve’s tightening measures and a broader rotation from growth to value-oriented sectors

AAPL’s product launch cycle presents another intriguing pattern. Pre-announcement rallies often occur (e.g., iPhone XS in September 2018, iPhone 11 in 2019, M1 MacBooks in November 2020), reflecting herd behaviour typical of mega-cap stocks. Investors frequently 'sell the news' on launch day, causing price reversion unless significant innovations or unexpected announcements are made.

Therefore, there may be periods of overbuying and overselling, improving the suitability of Bollinger Bands for AAPL. 


## Bollinger Bands

The bollinger signals file is first called in to the environment:

```{r, eval = FALSE}
library(readxl)
bollinger_signals <- read_excel("bollinger_signals.xlsx")
```

The Shiny UI is then simulated in the console: 
```{r, eval = FALSE}
# Step 4: Dynamic Visualization with Shiny
ui <- fluidPage(
    
    # Title
    titlePanel("Dynamic Bollinger Signals Visualization"),
    
    # Sidebar layout
    sidebarLayout(
        sidebarPanel(
            selectInput("stock", "Select Stock:", choices = names(signals_list)),
            dateRangeInput("date_range", "Select Date Range:", start = min(price_data$Date), end = max(price_data$Date))
        ),
        
        mainPanel(
            plotOutput("combined_plot"),
            DTOutput("signals_table")
        )
    )
)

server <- function(input, output, session) {
    
    filtered_data <- reactive({
        req(input$stock, input$date_range)
        data <- signals_list[[input$stock]]
        data <- data %>% filter(Date >= input$date_range[1] & Date <= input$date_range[2])
        return(data)
    })
    
    output$combined_plot <- renderPlot({
        req(filtered_data())
        data <- filtered_data()
        
        # Create a matplotlib-like plot
        par(mfrow = c(2, 1), mar = c(4, 4, 2, 1))
        
        # Plot price and Bollinger Bands
        plot(as.Date(data$Date), data$Price, type = "l", col = "blue", lwd = 2, xlab = "Date", ylab = "Price",
             main = paste("Bollinger Bands for", input$stock))
        lines(as.Date(data$Date), data$BB_Upper, col = "red", lwd = 1.5)
        lines(as.Date(data$Date), data$BB_Lower, col = "green", lwd = 1.5)
        lines(as.Date(data$Date), data$BB_Mean, col = "orange", lwd = 1.5)
        legend("topright", legend = c("Price", "BB Upper", "BB Lower", "BB Mean"), 
               col = c("blue", "red", "green", "orange"), lty = 1, cex = 0.8)
        
        # Plot Z-Statistic
        plot(as.Date(data$Date), data$Z_Statistic, type = "l", col = "purple", lwd = 2, xlab = "Date", ylab = "Z-Statistic",
             main = "Z-Statistic over Time")
        abline(h = c(-2, 2), col = "gray", lty = 2)
    })
    
    output$signals_table <- renderDT({
        req(filtered_data())
        datatable(filtered_data())
    })
}

shinyApp(ui, server)
```

```{r, echo=FALSE, out.width="75%"}
knitr::include_graphics("BollingerBandsShiny.png")
```

## Bollinger Bands simulation

The interface is divided into three key sections. The following code visualises the proportion of 'BUY' and 'SELL' signals across all S&P 500 constituents: 

```{r, eval = FALSE}
output$bar_chart <- renderPlotly({
    req(filtered_data())
    data <- filtered_data()
    signal_counts <- table(data$Signal[data$Signal %in% c("BUY", "SELL")])
    
    plot_ly(
        x = names(signal_counts),
        y = as.numeric(signal_counts),
        type = "bar",
        marker = list(color = c("#1abc9c", "#e74c3c"))
    ) %>% layout(
        title = paste("Signal Proportions (BUY/SELL) for", input$ticker),
        xaxis = list(title = "Signal"),
        yaxis = list(title = "Frequency")
    )
})
```

### Appropriateness of Bollinger Bands for AAPL - Discussion

```{r, echo=FALSE, out.width="80%"}
knitr::include_graphics("Buy-sell signals AAPL.png")
```

Firstly, there is a disparity in signal frequency throughout the almost 15-year simulation (262 "SELL" signals opposed to 150 "BUY" signals). The following theories have been proposed as to why this phenomena occurs... Efficient market hypothesis would suggest the frequent sell conditions are attributed to a pattern of overpricing and overbought conditions within the historical data, whereby prices revert to their mean after temporary deviations. However, given the continuous upwards trajectory in AAPL over the last decade, this suggests a trend rather than a purely efficient market (Malkiel, 2003). AAPL's price doesn't show dramatic drops that match the asymmetric volatility hypothesis during most of the time period, however, during the Covid-19 pandeic, there was a notable dip, supporting the hypothesis for periods of market stress. Generally, AAPL's market resilience dampens the theory's overall impact (Bekaert and Wu, 2000). Finally, loss aversion suggests that investors act defensively, prompting disproportionate SELL signals. This theory still struggles to explain the long-term trend. 

This reveals a  key limitation of Bollinger Bands is their underperformance in trending markets. During sustained uptrends (e.g., 2017–2019), prices consistently breached the upper band without accurately reflecting overbought conditions, ignoring market resilience and long-term growth. Instead, Bollinger Bands primarily interpret statistical anomalies. 

### Efficacy as a trading strategy

The following code underpins the succeeding images:

```{r, eval = FALSE}
output$line_plot <- renderPlotly({
    req(cumulative_profit())
    data <- cumulative_profit()
    
    plot_ly(data, x = ~Date, y = ~CumulativeProfit, type = "scatter", mode = "lines") %>%
        layout(
            title = paste("Cumulative Profit (£) for", input$ticker),
            xaxis = list(title = "Date"),
            yaxis = list(title = "Cumulative Profit (£)")
        )
})
```

The visualisation displays a line graph backtesting the financial performance of a given stock. The first image represents the default settings with 0% transaction costs.

```{r, echo=FALSE, out.width="75%"}
knitr::include_graphics("Cumprofit0.png")
```

0% is an important benchmark as it models the theoretical limit of the trading strategy without external distortions. However, in financial markets, the assumption of 0% transaction costs is entirely unrealistic. 

Transaction costs in financial markets are seen as residual losses, which are disaggregated into implicit and explicit costs (Allen, 1991). (Di Maggio et Al, 2020) gives a key distinction between the NASDAQ / NYSE and the LSE, whereby the American exchanges conduct a tiered fee structure based on monthly trading volume, whereas LSE operates on a fixed fee per trade. These fees are generally negligible, although when combined with other elements summate to a larger erosion of trader profitability. (Persaud, 2017) considers Stamp Duty Reserve Tax, relevant to the UK imposition of a 0.5% Stamp Duty on equity purchases on the London Stock Exchange (This doesn't apply to the Alternative Investments Market (AIM)).

Implicitly, the bid-ask spread is a key component of transaction costs (Copeland et Al, 1983). (Benston and Hagerman, 1974) maintain the magnitude of the bid-ask spread isn't fixed, rather a determinant of liquidity, volatility, trade size, and market structure. The following scrips were run in R to determine historical volumes traded and historical volatility: 

```{r, eval=FALSE}
library(quantmod)
symbol <- "AAPL"
getSymbols(symbol, src = "yahoo", from = "2010-01-01", to = Sys.Date(), auto.assign = TRUE)
historical_volumes <- AAPL[, "AAPL.Volume"]
print(head(historical_volumes))
print(tail(historical_volumes))
```
```{r, echo=FALSE}
knitr::include_graphics("Historicalvolumes.png")
```

AAPL has traded in the tens of millions since the financial crisis, indicative of a low bid-ask spread due to an extensive order book. 

```{r, eval=FALSE}
library(quantmod)
library(dplyr)
symbol <- "AAPL"
getSymbols(symbol, src = "yahoo", from = "2010-01-01", to = Sys.Date(), auto.assign = TRUE)

# Convert data to a data frame for easier handling
aapl_data <- data.frame(date = index(AAPL), coredata(AAPL))

# Daily returns
aapl_data <- aapl_data %>%
    mutate(daily_return = dailyReturn(Cl(AAPL)))

# Grouping
aapl_data <- aapl_data %>%
    mutate(year = format(date, "%Y"))

# Compute annualised volatility year by year
annual_volatility <- aapl_data %>%
    group_by(year) %>%
    summarize(
        std_dev_daily = sd(daily_return, na.rm = TRUE),
        annualized_volatility = std_dev_daily * sqrt(252)
    )

print(annual_volatility)
```
```{r, echo=FALSE}
knitr::include_graphics("Historicalvolatility.png")
```

Over the past 15 years, daily standard deviation and annualised volatility have remained relatively stable, with notable spikes during economic turmoil in 2020 and 2022. AAPL's high susceptibility to macroeconomic shocks suggests that moderate annualised volatility reflects investor compensation for holding risks, though it does not significantly impact the observed bid-ask spread.

Trade size is an adjustable parameter within the UI. As capital % increases, at 0% transaction cost, the y axis exponentiates to over 700 million by the end of the simulation. The inverse is seen if transaction costs were 2%. Position size may increase bid-ask spread as the sellers on the order book might not summate to the bigger buy order within the tighter spread. 

```{r, echo=FALSE, out.width="75%"}
knitr::include_graphics("Cumprofit1.png")
```

The trading strategy is highly sensitive to transaction costs, reflecting its high-turnover nature. Integrating Bollinger Bands with another strategy, such as MACD, could enhance profitability by reducing turnover. Future research should focus on identifying stylised properties to improve the efficacy of Bollinger Bands. 

```{r, echo=FALSE, out.width="75%"}
knitr::include_graphics("Cumprofit2.png")
```

# Regression Analysis

To gain deeper insights into Bollinger Bands, a predictive regression analysis was conducted, modelling future returns as the dependent variable and using the Z-statistic, proximity to the upper and lower bands, and band width as independent variables.

Random Forest Algorithms are utilised to capture the non-linearity and 'random walk' component of AAPL's stock price. Scripts for determining the optimal number of trees, number of predictors at each split, and minimum size of terminal nodes are computed in the Appendix. 

Three seperate models are created; 1-day returns as the dependent variable captures short-term market reactions; 5-day market returns captures medium-term efficacy, smoothing out noise, and 10-day market returns capture long-term durability. Please also see the Appendix for the full Random Forest model. 

Mathematically, they are denoted: 

## Econometric Equations for Model Testing

### 1-Day Return Model:

The regression model is estimated for 1-day returns:

\[
\text{return\_1d}_t = \beta_0 + \beta_1 \cdot \text{Band\_Width}_t + \beta_2 \cdot \text{Proximity\_to\_Upper}_t + \beta_3 \cdot \text{Proximity\_to\_Lower}_t + \beta_4 \cdot \text{VIX}_t + \beta_5 \cdot \text{Band\_Width\_Lag1}_t + \beta_6 \cdot \text{Z\_Statistic\_Lag1}_t + \beta_7 \cdot \text{Interaction\_BW\_VIX}_t + \epsilon_t
\]

Where:
- \(\text{Band\_Width}_t\) = the difference between the upper and lower Bollinger Bands at time \(t\);

- \(\text{Proximity\_to\_Upper}_t\) = distance of the price from the upper Bollinger Band at time \(t\),

- \(\text{Proximity\_to\_Lower}_t\) = distance of the price from the lower Bollinger Band at time \(t\),

- \(\text{VIX}_t\) = VIX index at time \(t\),

- \(\text{Band\_Width\_Lag1}_t\) = lagged value of Band Width (one time period earlier),

- \(\text{Z\_Statistic\_Lag1}_t\) = lagged value of the Z-Statistic (one time period earlier),

- \(\text{Interaction\_BW\_VIX}_t\) = interaction term between Band Width and VIX at time \(t\),

- \(\epsilon_t\) = error term at time \(t\).


### 5-Day Return Model:

\[
\text{return\_5d}_t = \beta_0 + \beta_1 \cdot \text{Band\_Width}_t + \beta_2 \cdot \text{Proximity\_to\_Upper}_t + \beta_3 \cdot \text{Proximity\_to\_Lower}_t + \beta_4 \cdot \text{VIX}_t + \beta_5 \cdot \text{Band\_Width\_Lag1}_t + \beta_6 \cdot \text{Z\_Statistic\_Lag1}_t + \beta_7 \cdot \text{Interaction\_BW\_VIX}_t + \epsilon_t
\]

### 10-Day Return Model:

\[
\text{return\_10d}_t = \beta_0 + \beta_1 \cdot \text{Band\_Width}_t + \beta_2 \cdot \text{Proximity\_to\_Upper}_t + \beta_3 \cdot \text{Proximity\_to\_Lower}_t + \beta_4 \cdot \text{VIX}_t + \beta_5 \cdot \text{Band\_Width\_Lag1}_t + \beta_6 \cdot \text{Z\_Statistic\_Lag1}_t + \beta_7 \cdot \text{Interaction\_BW\_VIX}_t + \epsilon_t
\]

These models aim to explain the returns over different time horizons (1-day, 5-day, 10-day) based on a combination of Bollinger Band metrics (width, proximity) and VIX.The lagged variables for Band Width and Z-Statistic allow the model to capture time-series dependencies. The interaction term between Band Width and VIX helps assess if the relationship between Bollinger Bands and returns is moderated by market volatility.

### Feature Engineering

```{r, eval = FALSE}
aapl_data <- aapl_data %>% 
    mutate(
        BB_Upper = na.locf(BB_Upper, na.rm = FALSE),
        BB_Lower = na.locf(BB_Lower, na.rm = FALSE),
        Band_Width = BB_Upper - BB_Lower,
        Proximity_to_Upper = BB_Upper - Price,
        Proximity_to_Lower = Price - BB_Lower,
        Band_Width_Lag1 = lag(Band_Width, 1),
        Z_Statistic_Lag1 = lag(Z_Statistic, 1),
        Interaction_BW_VIX = Band_Width * VIX,
        return_1d = (lead(Price, 1) - Price) / Price,
        return_5d = (lead(Price, 5) - Price) / Price,
        return_10d = (lead(Price, 10) - Price) / Price
    ) %>% 
    drop_na(Band_Width, Proximity_to_Upper, Proximity_to_Lower, 
            Band_Width_Lag1, Z_Statistic_Lag1, return_1d, return_5d, return_10d)
```
Key features were meticulously designed to capture distinct elements of market behavior: Band_Width quantified volatility, while Proximity_to_Upper and Proximity_to_Lower were constructed to evaluate price momentum and potential mean-reversion dynamics. Lagged variables, such as Band_Width_Lag1, incorporated the temporal dependence characteristic of volatility clustering. Furthermore, the inclusion of an interaction term (Interaction_BW_VIX) enabled the integration of Bollinger Band signals with market-wide sentiment, as represented by the VIX index. These features allowed for a nuanced exploration of Bollinger Bands across varying temporal horizons (1-day, 5-day, 10-day), aligning technical signals with macroeconomic context, thereby providing a robust foundation for evaluating their predictive validity within a financial framework. 

### Model Training

```{r, eval = FALSE}
final_rf_model_1d <- randomForest(
    return_1d ~ Band_Width + Proximity_to_Upper + Proximity_to_Lower + VIX +
        Band_Width_Lag1 + Z_Statistic_Lag1 + Interaction_BW_VIX,
    data = train_data,
    ntree = 500,
    mtry = tune_1d[1, "mtry"],
    importance = TRUE
)
final_rf_model_5d <- randomForest(...)  # Same structure for `return_5d`
final_rf_model_10d <- randomForest(...) # Same structure for `return_10d`
```

Hyperparameter tuning was performed systematically for each horizon using grid search (tuneRF) to optimize key parameters, such as the number of features (mtry) and the number of trees (ntree), ensuring the models were neither overfitted nor underpowered. The inclusion of parallel processing enhanced computational efficiency, enabling the exploration of complex relationships between engineered features and target variables.

The training incorporated key technical indicators, macroeconomic predictors (e.g., VIX), and interaction terms to model non-linear dependencies and volatility clustering. By aligning the training process with financial theory and methodological rigor, the resulting models provided a comprehensive framework to test whether Bollinger Bands possess predictive validity across varying time frames in dynamic financial markets. This methodological rigor ensured the findings were both statistically robust and economically meaningful.

## Plots
The plots are seen below, with predicted returns on the X axis, and actual returns on the Y axis. 

```{r, echo=FALSE, out.width="75%"}
knitr::include_graphics("ActualPredicted.png")
```

From the images above, the 1-day model has less scatter around the trend line, implying the model captures the daily variations in price more effectively compared to the 5 and 10-day model. With the 5-day model, we begin to see more dispersion. From a theoretical standpoint, stock returns are more likely to be influenced by other external factors not captured in the model such as sentiment and other external factors. Within the 10-day model, the issue of noise and market complexity overwhelmed the Random Forest's ability to capture volatility, indicative of higher uncertainty. 

### SHAP, ALE analysis

```{r, eval = FALSE}
predictor <- Predictor$new(final_rf_model_1d, data = X_train, y = train_data$return_1d)
shap_values <- Shapley$new(predictor, x.interest = X_train[1, , drop = FALSE])
shap_values$plot()

pdp <- FeatureEffect$new(predictor, feature = "Band_Width", method = "ale")
pdp$plot()
```

Full code for these values are found in the Appendix.  

The magnitude of phi represents the strength of the feature's contribution to the prediction; phi.var represents the variance of the SHAP values across all data points for a particular feature, meaning that a high variance indicates a larger spread on its impact on predictions across observations; the feature value refers to the actual value of the feature for the variable one is examining, contextualising the SHAP value. 

Firstly, band width shows a consistently positive contribution, increasing with the horizon prediction (1-day = 0.0016, 10-day 0.0114), suggesting its growing importance in longer-term forecasts. Band width lagged by a day shows increasing variability with time (1-day: 1.45x10^-5, 10-day:2.3x10^-4), indicating its growing influence but context-dependent effect, supporting volatility clustering theory (Cont, 2007). Proximity_to_lower exhibited positive contributions in all models, reinforcing its role in signaling mean-reversion behaviour. However, the phi values of proximity_to_higher are also all positive (1-day: 7.6x10^-4, 10-day:1.72x10^-3), suggesting AAPL gained momentum in the backtest, rather than reverting to the mean, reinforcing the upwards trend. The phi and phi.var values for VIX remain relatively stable, indicating their consistent impact as a metric on financial markets. Variance magnitude remains low, suggesting low context-dependence around this predictor. 

PDP's reveal how band width influences the target outcome, highlighting non-linearity, aiming to illustrate marginal effects. Accumulated Local Effects (ALE) adjusts for feature correlations, enhancing interpretability. ALE's are also much more computationally efficient, perfect for our larger dataset. Each row computes the .value, representing the predicted impact of a specific band width on returns. The band_width is the feature value, in which the ALE value is computed. Each row corresponds to a range of band width values, and the ALE shows the average effect of the feature in that interval. In the 1-day model, .value initates as positive with low band-width (e.g .0033 at Band_width 0.327), although this gradually decreases. In stable market conditions, the model predicts positive returns, supporting that low volatility supports predictable price trends. In moderate band width for the 1-day model, .value transitions from slightly positive to slightly negative (-.00032 at Band Width 4.3), meaning the positive impact of band width diminishes as volatility increases. .value becomes negative in high band width conditions, and continues to decrease with increasing band width, until it stabilises around Band_width = 25 at -0.0013. In highly volatile markets, Bollinger Band width negatively impacts returns. High volatility reduces predicatability, making short-term returns harder to capture. 

The positive effects become stronger the longer the time-horizon of the model for low band width, however the negative effects appear earlier in the 5 and 10-day models, reflecting the compounding risk of volatility over longer timeframes. Interestingly, the high bandwidth has the most impact in the 5-day model, whereas in the 10-day model, this negative impact stabilises, suggesting extreme volatility has diminishing marginal effects over longer periods. Please see the Appendix for the appropriate code for the Excel file containing the values.  

The following table comments on model performance: 

```{r, echo=FALSE}
knitr::include_graphics("Evaluation Metrics.png")
```
### Evaluation Metrics

```{r, eval = FALSE}
evaluate_metrics <- function(predictions, actuals, threshold = 0) {
    directional_accuracy <- mean(sign(predictions) == sign(actuals))
    hit_rate <- mean(predictions > threshold & actuals > threshold)
    sharpe_ratio <- SharpeRatio(predictions - actuals, Rf = 0.02 / 252, p = 0.95)
    return(data.frame(Directional_Accuracy = directional_accuracy,
                      Hit_Rate = hit_rate,
                      Sharpe_Ratio = sharpe_ratio))
}

metrics_1d <- evaluate_metrics(predict(final_rf_model_1d, test_data), test_data$return_1d)
metrics_5d <- evaluate_metrics(...)  # Same for `return_5d`
metrics_10d <- evaluate_metrics(...) # Same for `return_10d`
```

Directional accuracy measures the percentage of correct predictions regarding the direction of price movement (up or down). The directional accuracy is 52% for the 1 and 5 day models, whereas it's 47% for the 10 day model, indicative of little predictive power. For comparison, a random guess would have 50%, thus any percentage above 50% indicates some predictive power, albeit not very high. 

The hit rate measures the percentage of predictions where the model’s forecasted return exceeds a specified threshold (set at 0 to signify positive returns). A higher hit rate indicates that the model tends to predict positive returns correctly. As the number is .39 for the 1-day model, but .13 for the 5 and 10 day models respectively, this indicates Bollinger Bands with the random forest algorithm is better at predicting returns in the short-term. 

The Sharpe ratio measures the excess return (over the risk-free rate) per unit of volatility (standard deviation), a key measure of risk-adjusted performance. The 1-day return (.29 Sharpe Ratio) implies the model earns returns above the risk-free rate, adjusted to its volatility, however, a Sharpe ratio under 1 isn't sufficient for an algorithm, due to the poor risk-adjusted return relative to volatility. As the model moves into the 5 and 10 day return horizons, the Sharpe ratio turns negative (.26 and .56 respectively), suggesting a risk-adjusted loss. 

Whilst StDev Sharpe looks at the overall volatility of the Bollinger Bands strategy, the other three metrics provide a comprehensive view of risk. Please see the Appendix. 

# Conclusion

Bollinger Bands serve as an effective tool for determining oscillations in a stationary stock. However, they should not be the sole driver of an algorithmic trading strategy due to the high turnover generated through frequent position changes. The frequency of these position changes entirely erodes cumulative profit at 1.155% for AAPL, which underscores a key tradeoff - The inverse relationship between risk-adjusted returns and turnover from high-frequency trading. The longer the model was tested, the greater struggle the model had capturing volatility, calling into question additional model features. Throughout efficacy testing, the 1-day return model consistently performed better than the 5 and 10-day model, indicative of greater immediate predictive power.  Detailed transaction cost modelling would provide a clearer picture to the exact efficacy of AAPL during the sample period. All models struggle to provide returns under more volatile conditions.

# Bibliography

Bollinger, J., 1992. Using bollinger bands. Stocks & Commodities, 10(2), pp.47-51. 

Chen, S., Zhang, B., Zhou, G., & Qin, Q. (2018). Bollinger Bands Trading Strategy Based on Wavelet Analysis. Applied Economics and Finance, 5(3), pp. 49–57. Available at: https://doi.org/10.11114/aef.v5i3.3079

Yan, K., Wang, Y. and Li, Y., 2023. Enhanced Bollinger Band Stock Quantitative Trading Strategy Based on Random Forest. Artificial Intelligence Evolution, pp.22-33.

Ravichandra, T. and Hanif, M., 2015. Bollinger bands optimal algorithmic strategyinstock trading. International Journal of Research in Finance and Marketing, 5(1), pp.1-9.

Darmawan, O. A., Heryadi, Y., Lukas, Wulandhari, L. A., & Sonata, I. (2024). The Utilization of Fuzzy Logic and Bollinger Bands to Enhance Trading Decision-Making During the Bitcoin Halving Phase. Procedia Computer Science, 245, pp. 272–281. Available at: https://www.sciencedirect.com/science/article/pii/S2452452024000272

H. Chlif, D. Kanzari and Y. Ben Said, "An Adaptive Neuro Fuzzy to Predict Cryptocurrency Based on the Crisp Method: Case of COVID-19," 2023 IEEE International Conference on Advances in Data-Driven Analytics And Intelligent Systems (ADACIS), Marrakesh, Morocco, 2023, pp. 1-6, doi: 10.1109/ADACIS59737.2023.10424284.

Lento, C., Gradojevic, N. and Wright, C.S., 2007. Investment information content in Bollinger Bands?. Applied Financial Economics Letters, 3(4), pp.263-267.

Torrence, C. and Compo, G.P., 1998. A practical guide to wavelet analysis. Bulletin of the American Meteorological society, 79(1), pp.61-78.

Manimaran, P., Panigrahi, P.K. and Parikh, J.C., 2005. Wavelet analysis and scaling properties of time series. Physical Review E—Statistical, Nonlinear, and Soft Matter Physics, 72(4), p.046120.

Jansen, M., 2012. Noise reduction by wavelet thresholding (Vol. 161). Springer Science & Business Media.

Breiman, L., 2001. Random forests. Machine learning, 45, pp.5-32.

Biau, G. and Scornet, E., 2016. A random forest guided tour. Test, 25, pp.197-227.

Lin, Y. and Jeon, Y., 2006. Random forests and adaptive nearest neighbors. Journal of the American Statistical Association, 101(474), pp.578-590.

Zadeh, L.A., 1988. Fuzzy logic. Computer, 21(4), pp.83-93.

Lauguico, S., Concepcion II, R., Alejandrino, J., Macasaet, D., Tobias, R.R., Bandala, A. and Dadios, E., 2019, November. A fuzzy logic-based stock market trading algorithm using bollinger bands. In 2019 IEEE 11th international conference on humanoid, nanotechnology, information technology, communication and control, environment, and management (HNICEM) (pp. 1-6). IEEE.

Cheung, W.M. and Kaymak, U., 2007, November. A fuzzy logic based trading system. In Proceedings of the Third European Symposium on Nature-inspired Smart Information Systems (Vol. 59, pp. 1-60).

Thomas, D.M. and Mathur, S., 2019, June. Data analysis by web scraping using python. In 2019 3rd International conference on Electronics, Communication and Aerospace Technology (ICECA) (pp. 450-454). IEEE.

Tetlock, P.C., 2007. Giving content to investor sentiment: The role of media in the stock market. The Journal of finance, 62(3), pp.1139-1168.

Malkiel, B.G., 2003. The efficient market hypothesis and its critics. Journal of economic perspectives, 17(1), pp.59-82.

Bekaert, G. and Wu, G., 2000. Asymmetric volatility and risk in equity markets. The review of financial studies, 13(1), pp.1-42.

Allen, D.W., 1991. What are transaction costs?. Rsch. in L. & Econ., 14, p.1.

Copeland, T.E. and Galai, D., 1983. Information effects on the bid‐ask spread. the Journal of Finance, 38(5), pp.1457-1469.

Cont, R., 2007. Volatility clustering in financial markets: empirical facts and agent-based models. In Long memory in economics (pp. 289-309). Berlin, Heidelberg: Springer Berlin Heidelberg.

# Appendix

## Wavelet analysis

Wavelet analysis is a mathematical tool for analysing non-stationary and noisy time-series data. Unlike Fourier Transform, which provides frequency-domain information without time localisation, wavelet analysis offers a multi-resolution approach, allowing for simultaneous examination of time and frequency components; theorised to be effective in financial markets, whereby trends and patterns occur at varying scales.

As wavelet analysis decomposes a time-series into components at multiple scales using wavelet basis functions, a wavelet function satisfies the following properties:

### Zero Mean

$$
\int_{-\infty}^{\infty} \varphi(t) dt = 0
$$

This property ensures the wavelet function has equal positive and negative values, which cancel each other out when integrated across the entire time domain. This allows wavelets to detect changes or localised features (like sharp transitions) in the data, as opposed to representing constant trends.

### Finite Energy

$$
\int_{-\infty}^{\infty} |\varphi(t)|^2 dt < \infty
$$

This property ensures the wavelet function is localised in time, meaning it is significant within a finite region, which diminishes rapidly outside that region. This is ideal for analysing signals with transient changes, which exist in financial time-series (Torrence and Compo, 1998).

Based on this theoretical foundation, wavelet decomposition simply decomposes signal $v(t)$ into low-frequency (trend) and high-frequency (detail or noise) components.

Scaling coefficients represent the smooth, low-frequency components of the signal, where $\varphi_{jk}(t)$ is the scaling function.

Wavelet coefficients represent the fine, high-frequency components capturing abrupt changes, whereby $\varphi_{jk}(t)$ is the wavelet function.

$$
c_{jk} = \langle v(t), \varphi_{jk}(t) \rangle
$$

The signal can be expressed as:

$$
v(t) = \sum_k c_{jk} \varphi_{jk}(t) + \sum_{j=j}^{\infty} \sum_k d_{jk} \varphi_{jk}(t)
$$

Hard thresholding is applied for noise reduction. Noise is effectively removed whilst retaining significant trends. This method preserves larger coefficients, whilst retaining sharp transitions in data, whereas discontinuities may be introduced at the threshold boundary. In contrast, soft thresholding shrinks coefficients towards zero by subtracting the threshold value. This helps yield smoother results and avoid discontinuities, yet this might distort the data for smaller coefficients. This area could be explored in the future to optimise data structures (Manimaran et al., 2005).

$$
T_{\text{thr}} = \sqrt{2 \log(n)} \cdot \sigma
$$

After thresholding, the filtered signal is reconstructed by combining the remaining coefficients (Jensen, 2012):

$$
v_{\text{filtered}}(t) = \sum_k c_{jk} \varphi_{jk}(t) + \sum_{j=j}^{\infty} \sum_k d_{jk} \varphi_{jk}(t)
$$

## Random Forest Algorithm

The advent of large datasets in modern applications necessitates learning algorithms that scale effectively with data volume while maintaining robust statistical performance. Random Forests, developed by (Breiman, 2001), are among the most successful methods for handling such data. As a supervised learning procedure, Random Forests build an ensemble of decision trees using the "divide and conquer" principle: splitting the dataset into random subsets, training individual predictors on these subsets, and aggregating their outputs.

Whilst Random Forests are empirically effective, their theoretical underpinnings are much less developed (Biau & Scornet, 2016). (Breiman, 2001) laid out the foundation for Random Forests, providing an upper bound on the generalisation error, acknowledging that tree correlation through randomisation improves ensemble performance, whilst pertaining to the limited theoretical understanding of these interactions. (Lin & Jeon, 2006) explore the connection between Random Forests and nearest neighbour predictions.

## Fuzzy Logic

Fuzzy logic is a form of reasoning that deals with approximate rather than fixed reasoning. Unlike binary logic (0 and 1), it operates on degrees of truth (Zadeh, 1988). In the context of Bollinger Bands, the variables derived from Bollinger Bands are converted into fuzzy inputs such as distance from the bands, and band width. Membership functions represent the degree of membership of a variable in linguistic categories (for example: “low”, “medium”, and “high”). Membership functions define how each point in the input space is mapped to a degree of membership within the fuzzy set.

These degrees are represented as values between 0 and 1, where 0 means no membership, and 1 means full membership. The key purpose of this is to handle uncertainty and imprecision in data. The most common membership function for stock market data would be the Gaussian membership function as it’s argued that this type of distribution mimics natural distributions (Lauguico, 2019), (Darmawan, 2024). The Gaussian membership function takes the form:

$$
\mu_A(x) = e^{\frac{-(x-c)^2}{2\sigma^2}}
$$

The two parameters are:

- **c** – Center  
- **$\sigma$** – Standard Deviation  

It’s important to note the parameters “low”, “medium”, “high” take the Triangular membership function.

After this, trading signals are based on the fuzzy inputs; rules are constructed using the “IF-THEN” format. For example:

> *“If the price is very close to the lower band AND the band width is wide, THEN generate a strong buy signal.”*

> *“If the price is moderately close to the upper band AND the band width is narrow, THEN generate a weak sell signal.”*

A fuzzy inference system processes the fuzzy inputs, applies the rules to determine the output. Bollinger Bands models tend to use either the Sugeno or Mamdani fuzzy inference system (Cheung and Kaymak, 2007). The Mamdani fuzzy inference system then leads to defuzzification.

## Contrarian Framework

In Leyman’s terms, a contrarian approach is whereby investors act opposite to prevailing market trends or sentiment. Popular market opinions lead to mispricing of assets due to herd behaviour and emotional decision-making. An example of an individual who uses the Contrarian framework would be one who buys low and sells high.

## Data Preprocessing

### Macroeconomic Variables

```{r, eval = FALSE}
#TRAINING/TESTING DATA

#Pulling in daily inflation data - Macroeconomic Data - Inflation-adjusted expectations. 
#Derived from the difference between nominal Treasury yields and inflation-protected bonds (TIPS)
library(fredr)
fredr_set_key("3c907578364cac27c9d8e6b1c7630cb1")
breakeven_10yr <- fredr_series_observations(
    series_id = "T10YIE",
    observation_start = as.Date("2010-01-01"),
    observation_end = Sys.Date()
)    

#Risk-free rate - Nominal growth expectations
treasury_10yr <- fredr_series_observations(
  series_id = "DGS10",            
  observation_start = as.Date("2010-01-01"), 
  observation_end = Sys.Date()     
)

#U/e
unemployment_rate <- fredr_series_observations(
  series_id = "UNRATE",
  observation_start = as.Date("2010-01-01"),
  observation_end = Sys.Date()
)

#Creating daily dataset using interpolation from monthly data
start_date <- as.Date("2010-01-01")
end_date <- Sys.Date() 
daily_dates <- seq(from = start_date, to = end_date, by = "day")
daily_unemployment <- data.frame(date = daily_dates)
daily_unemployment <- left_join(daily_unemployment, unemployment_rate, by = "date")
daily_unemployment <- daily_unemployment %>% select(-c(2, 4, 5))
daily_unemployment$value <- na.approx(daily_unemployment$value, rule = 2)


#Interest rate data - 10 year Treasury yield > Fed funds rate 
#Rationale - Short-term volatility in Fed funds rate as traders aim to rebalance portfolios. 

#Treasury yield > Breakeven rate: Could indicate expectations for real economic growth without significant inflationary pressures.
#Treasury yield ≈ Breakeven rate: Suggests that most of the yield is being eroded by inflation expectations, indicating the market anticipates little real growth.
#Treasury yield < Breakeven rate: This is rare but could signal stagflationary conditions, where inflation expectations outpace growth expectations.
#Having both variables enables your model to understand not only the level of interest rates and inflation expectations but also their relative dynamics

#Quantmod , volatility index
getSymbols("^VIX", src = "yahoo", from = "2010-01-01", to = Sys.Date())
VIX <- VIX[, ncol(VIX)]

#Merging macro-level data (applied to everything)
VIX <- data.frame(date = index(VIX), coredata(VIX))
breakeven_10yr$date <- as.Date(breakeven_10yr$date)
treasury_10yr$date <- as.Date(treasury_10yr$date)
daily_unemployment$date <- as.Date(daily_unemployment$date)
VIX$date <- as.Date(VIX$date)
combined_data <- breakeven_10yr %>%
  left_join(treasury_10yr, by = "date") %>%
  left_join(daily_unemployment, by = "date") %>%
  left_join(VIX, by = "date")
print(head(combined_data, 10), width = Inf)

#Cleaning table
combined_data <- combined_data %>%
  select(-c(2, 4, 5, 6, 8, 9)) %>%
  rename(
    breakeven_10yr = value.x,
    treasury_10yr = value.y,
    ue_rate = value
  )
```

### S&P500 Constituents - adjusted close price data

```{r, eval = FALSE}
#Bloomberg Terminal (Physical terminal in TPSC1.03)
install.packages("Rblpapi")
library(Rblpapi)
blpConnect()

# Pulling all constituents of S&P500
sp500_constituents <- bds("SPX Index", "INDX_MEMBERS")
securities <- sub(" [A-Z]{2}$", "", sp500_constituents$`Member Ticker and Exchange Code`)
head(securities)
df <- data.frame(Ticker = securities, stringsAsFactors = FALSE)
df$ticker_full <- paste(df$Ticker, "Equity", sep = " ")
bbgid_data <- bdp(securities = df$ticker_full, fields = "ID_BB_GLOBAL")
df$bbgid <- bbgid_data$ID_BB_GLOBAL

head(df)

#Historical S&P500 data
start_date <- as.Date("2010-01-01")
end_date <- Sys.Date()
fields <- c("PX_OPEN", "PX_HIGH", "PX_LOW", "PX_LAST")
sp500_ohlc <- bdh(securities = "SPX Index", 
                  fields = fields, 
                  start.date = start_date, 
                  end.date = end_date)
head(sp500_ohlc)

ticker_full <- paste(securities, "US Equity")
price_data <- bdh(
  securities = ticker_full,         
  fields = "PX_CLOSE",              
  start.date = start_date,
  end.date = end_date
)


#test
test_data <- bdh(
    securities = "AAPL US Equity",
    fields = "PX_LAST",
    start.date = start_date,
    end.date = end_date
)
print(test_data)
tail(test_data)


# Define the full list of S&P 500 tickers in Bloomberg format
# Assuming `ticker_full` is your predefined list of tickers in "US Equity" format

# Define batch size and split `ticker_full` into batches of 100
batch_size <- 100
ticker_batches <- split(ticker_full, ceiling(seq_along(ticker_full) / batch_size))

# Initialize an empty list to store data for each batch
all_data <- list()

# Loop through each batch
for (i in seq_along(ticker_batches)) {
    batch <- ticker_batches[[i]]
    batch_data <- list()  # Temporary list to store each ticker's data within the batch
    
    # Loop through each ticker in the current batch
    for (ticker in batch) {
        # Fetch the adjusted close prices for the current ticker
        ticker_data <- bdh(
            securities = ticker,
            fields = "PX_LAST",  
            start.date = start_date,
            end.date = end_date
        )
        
        # Store the result in the batch_data list with the ticker as the name
        batch_data[[ticker]] <- ticker_data
        
        # Pause for 2 seconds before the next request
        Sys.sleep(2)
    }
    
    # Combine the batch data and add it to all_data
    all_data[[i]] <- bind_rows(batch_data, .id = "ticker")
}

# Combine all batches into a single data frame
price_data <- bind_rows(all_data)

# Optionally, reshape to have dates as rows and tickers as columns
price_data_wide <- price_data %>%
    pivot_wider(names_from = ticker, values_from = PX_LAST) %>%
    rename(Date = date)

# Display the head of the final combined data
print(head(price_data_wide))

save.image(file = "Machine_learning_noob.RData")
summary(price_data_wide)
dim(price_data_wide)

# Load required library
library(openxlsx)

# Load the data from your specified file path
load("C:/Users/jakes/Downloads/Machine_learning_noob.RData")

# Check if 'price_data_wide' is loaded successfully
if (exists("price_data_wide")) {
  # Create a new workbook
  wb <- createWorkbook()
  
  # Add a worksheet named "S&P500 Data"
  addWorksheet(wb, "S&P500 Data")
  
  # Write the entire dataframe to the Excel worksheet
  writeData(wb, "S&P500 Data", price_data_wide, startCol = 1, startRow = 1)
  
  # Save the workbook as an Excel file
  saveWorkbook(wb, "C:/Users/jakes/Downloads/SP500_Data.xlsx", overwrite = TRUE)
  
  # Confirm output
  cat("Excel file saved as 'C:/Users/jakes/Downloads/SP500_Data.xlsx'")
} else {
  cat("Error: 'price_data_wide' not found in the loaded RData file.")
}
```

## Causality Analysis

![Causality Screenshot](Causality.png)

Please see the full code below: 

```{r, eval = FALSE}
library(shiny)
library(shinythemes)
library(plotly)
library(dplyr)
library(tseries)
library(vars)  # For Granger Causality
library(urca)  # For Cointegration test
library(colourpicker)
library(strucchange)  # For structural breaks (using the CUSUM test or Bai-Perron)

# Check if `price_data_wide` exists
if (!exists("price_data_wide")) {
    stop("The dataset `price_data_wide` is missing. Please load the correct data.")
}

# Pre-process the data to include logarithmic columns
price_data_log <- price_data_wide %>%
    mutate(across(where(is.numeric), ~log(.), .names = "log_{col}"))

# Define UI
ui <- fluidPage(
    uiOutput("theme_ui"),
    titlePanel("View Stock"),
    sidebarLayout(
        sidebarPanel(
            selectInput("selected_column", "Choose a column to plot:",
                        choices = names(price_data_wide)[-1],
                        selected = names(price_data_wide)[2]),
            selectInput("comparison_column", "Choose a second column to plot (optional):",
                        choices = c("None", names(price_data_wide)[-1]),
                        selected = "None"),
            selectInput("selected_year", "Choose a year to display:",
                        choices = c("Max", unique(format(price_data_wide$Date, "%Y"))),
                        selected = "Max"),
            selectInput("theme", "Choose Theme:",
                        choices = c("Journal" = "journal", "Superhero" = "superhero"),
                        selected = "journal"),
            colourInput("primary_color", "Select Primary Plot Color:", value = "#0000FF"),
            colourInput("comparison_color", "Select Comparison Plot Color:", value = "#FF0000"),
            br(),
            actionButton("causality_check", "Check for Causality")
        ),
        mainPanel(
            plotlyOutput("graph")
        )
    )
)

server <- function(input, output, session) {
    # Dynamically update the theme
    output$theme_ui <- renderUI({
        fluidPage(theme = shinytheme(input$theme))
    })
    
    # Interactive Plot
    output$graph <- renderPlotly({
        filtered_data <- price_data_wide %>% 
            filter(!is.na(.data[[input$selected_column]])) %>% 
            {
                if (input$selected_year != "Max") {
                    filter(., format(Date, "%Y") == input$selected_year)
                } else {
                    .
                }
            }
        plot <- plot_ly(data = filtered_data) %>% 
            add_lines(
                x = ~Date,
                y = ~.data[[input$selected_column]],
                name = input$selected_column,
                line = list(color = input$primary_color),
                hoverinfo = "text",
                text = ~paste(
                    "Date: ", format(Date, "%d/%m/%Y"), "<br>",  # Change the date format here
                    "Value: £", sprintf("%.2f", .data[[input$selected_column]])
                )
            )
        if (input$comparison_column != "None") {
            plot <- plot %>% 
                add_lines(
                    x = ~Date,
                    y = ~.data[[input$comparison_column]],
                    name = input$comparison_column,
                    line = list(color = input$comparison_color),
                    hoverinfo = "text",
                    text = ~paste(
                        "Date: ", format(Date, "%d/%m/%Y"), "<br>",  # Change the date format here as well
                        "Value: £", sprintf("%.2f", .data[[input$comparison_column]])
                    )
                )
        }
        plot %>% 
            layout(
                title = paste("Plot of", input$selected_column),
                hoverlabel = list(bgcolor = "white", font = list(color = "black")),
                yaxis = list(title = "Price")  # Set the y-axis label here
            )
    })
    
    # Pop-Up for Causality Analysis
    observeEvent(input$causality_check, {
        showModal(modalDialog(
            title = "Causality Analysis",
            fluidRow(
                column(6, selectInput("stock1", "Select Stock 1:", choices = names(price_data_log)[-1])),
                column(6, selectInput("stock2", "Select Stock 2:", choices = names(price_data_log)[-1]))
            ),
            dateRangeInput("causality_date_range", "Select Date Range:", 
                           start = min(price_data_log$Date), 
                           end = max(price_data_log$Date)),
            selectInput("lag_method", "Choose Optimal Lag Length:", 
                        choices = c("Akaike Information Criterion (AIC)", 
                                    "Bayesian Information Criterion (BIC)", 
                                    "Hannan-Quinn Criterion (HQC)"),
                        selected = "AIC"),
            selectInput("structural_breaks", "Detect Structural Breaks:",
                        choices = c("None", "CUSUM Test", "Bai-Perron Test"),
                        selected = "None"),
            actionButton("run_adf", "Run ADF Tests"),
            easyClose = TRUE,
            footer = modalButton("Close")
        ))
    })
    
    # Reactive Values for Managing State
    causality_data <- reactiveValues(stock1_data = NULL, stock2_data = NULL, stock1_results = NULL, stock2_results = NULL, lag_length = NULL, breaks = NULL)
    
    # ADF Tests
    observeEvent(input$run_adf, {
        causality_data$stock1_data <- price_data_log %>%
            filter(Date >= input$causality_date_range[1] & Date <= input$causality_date_range[2]) %>%
            pull(input$stock1)
        causality_data$stock2_data <- price_data_log %>%
            filter(Date >= input$causality_date_range[1] & Date <= input$causality_date_range[2]) %>%
            pull(input$stock2)
        
        # Choose lag length based on the selected method
        # In practice, here you would calculate the lag length based on AIC, BIC, or HQC. For now, we mock it.
        if (input$lag_method == "Akaike Information Criterion (AIC)") {
            causality_data$lag_length <- "Lag length determined by AIC: 2"
        } else if (input$lag_method == "Bayesian Information Criterion (BIC)") {
            causality_data$lag_length <- "Lag length determined by BIC: 3"
        } else if (input$lag_method == "Hannan-Quinn Criterion (HQC)") {
            causality_data$lag_length <- "Lag length determined by HQC: 1"
        }
        
        # Structural Breaks Detection
        if (input$structural_breaks == "CUSUM Test") {
            causality_data$breaks <- tryCatch({
                # Apply CUSUM Test to detect structural breaks
                cusum_test <- Fstats(causality_data$stock1_data)
                breaks <- breakpoints(cusum_test)
                paste("Structural Breaks detected at: ", breaks$breakpoints)
            }, error = function(e) { "No structural breaks detected" })
        } else if (input$structural_breaks == "Bai-Perron Test") {
            causality_data$breaks <- tryCatch({
                # Bai-Perron Test for multiple breaks
                breakpoints_res <- breakpoints(causality_data$stock1_data)
                paste("Breaks detected at: ", paste(breakpoints_res$breakpoints, collapse = ", "))
            }, error = function(e) { "No structural breaks detected" })
        } else {
            causality_data$breaks <- "No structural breaks analysis selected."
        }
        
        run_adf <- function(data) {
            differences <- 0
            while (TRUE) {
                test <- adf.test(data, k = 0)
                if (test$p.value < 0.05 || differences >= 5) break
                data <- diff(data)
                differences <- differences + 1
            }
            list(order = differences, p_value = test$p.value, statistic = test$statistic)
        }
        
        causality_data$stock1_results <- run_adf(causality_data$stock1_data)
        causality_data$stock2_results <- run_adf(causality_data$stock2_data)
        
        showModal(modalDialog(
            title = "ADF Test Results",
            HTML(paste0(
                "<b>", input$stock1, ":</b> Order of Integration: ", causality_data$stock1_results$order, 
                " (P-Value: ", round(causality_data$stock1_results$p_value, 4), ")<br>",
                "<b>", input$stock2, ":</b> Order of Integration: ", causality_data$stock2_results$order, 
                " (P-Value: ", round(causality_data$stock2_results$p_value, 4), ")<br><br>",
                "<b>Lag Length Determination:</b><br>",
                causality_data$lag_length,
                "<br><br><b>Structural Breaks Analysis:</b><br>",
                causality_data$breaks
            )),
            footer = tagList(
                modalButton("Close"),
                if (causality_data$stock1_results$order == causality_data$stock2_results$order) {
                    actionButton("cointegration_test", "Test for Cointegration")
                } else {
                    actionButton("granger_test", "Granger Causality")
                }
            )
        ))
    })
    
    # Cointegration Test
    observeEvent(input$cointegration_test, {
        coint_data <- cbind(
            stock1 = as.numeric(causality_data$stock1_data),
            stock2 = as.numeric(causality_data$stock2_data)
        )
        
        coint_data <- na.omit(coint_data)
        if (!is.matrix(coint_data) || ncol(coint_data) != 2 || any(!is.finite(coint_data))) {
            showModal(modalDialog(
                title = "Error",
                "Cointegration test requires two valid numeric time series with no missing values.",
                easyClose = TRUE,
                footer = modalButton("Close")
            ))
            return(NULL)
        }
        
        tryCatch({
            coint_test <- ca.jo(coint_data, type = "trace", ecdet = "const", K = 2)
            showModal(modalDialog(
                title = "Cointegration Test Results",
                HTML(paste0(
                    "Test Statistic: ", round(summary(coint_test)@teststat[1], 4), "<br>",
                    "Critical Value: ", round(summary(coint_test)@cval[1, 1], 4), "<br>",
                    ifelse(summary(coint_test)@teststat[1] > summary(coint_test)@cval[1, 1], 
                           "Cointegrated", "Not Cointegrated")
                )) ,
                footer = modalButton("Close")
            ))
        }, error = function(e) {
            showModal(modalDialog(
                title = "Error",
                "An error occurred while running the Cointegration test. Please ensure the data is valid.",
                easyClose = TRUE,
                footer = modalButton("Close")
            ))
        })
    })
    
    # Granger Causality Test
    observeEvent(input$granger_test, {
        tryCatch({
            var_model <- VAR(cbind(causality_data$stock1_data, causality_data$stock2_data), p = 2)
            granger_y1_cause_y2 <- causality(var_model, cause = "y1")
            granger_y2_cause_y1 <- causality(var_model, cause = "y2")
            
            p_value_y1_cause_y2 <- granger_y1_cause_y2$Granger[["p.value"]]
            p_value_y2_cause_y1 <- granger_y2_cause_y1$Granger[["p.value"]]
            
            relationship <- if (p_value_y1_cause_y2 < 0.05 & p_value_y2_cause_y1 < 0.05) {
                "Bidirectional Relationship"
            } else if (p_value_y1_cause_y2 < 0.05) {
                "Unidirectional: Stock 1 Granger-causes Stock 2"
            } else if (p_value_y2_cause_y1 < 0.05) {
                "Unidirectional: Stock 2 Granger-causes Stock 1"
            } else {
                "No Granger Causal Relationship"
            }
            
            showModal(modalDialog(
                title = "Granger Causality Test Results",
                HTML(paste0(
                    "<b>Granger Causality Test Summary:</b><br>",
                    "<b>Stock 1 -> Stock 2:</b> P-Value = ", round(p_value_y1_cause_y2, 4), "<br>",
                    "<b>Stock 2 -> Stock 1:</b> P-Value = ", round(p_value_y2_cause_y1, 4), "<br><br>",
                    "<b>Conclusion:</b> ", relationship
                )) ,
                footer = modalButton("Close")
            ))
        }, error = function(e) {
            showModal(modalDialog(
                title = "Error",
                "An error occurred during the Granger Causality Test. Please check the data and try again.",
                easyClose = TRUE,
                footer = modalButton("Close")
            ))
        })
    })
}

shinyApp(ui = ui, server = server)
```


## Article Scraper

The rapid pace of information dissemination in modern financial markets underscores the importance of timely and accurate data aggregation. An article scraper, a tool designed to automatically extract and consolidate news articles, offers significant benefits across various domains. While its utility in simplifying equity research is evident, the tool also provides unparalleled advantages from a qualitative data perspective

One of the primary qualitative benefits of an article scraper is its ability to identify emerging trends across industries and sectors. Academic literature on trend detection often highlights the importance of synthesising diverse data sources to discern patterns that may signal market opportunities (Thomas Et Al, 2019). For example, tracking keywords or thematic shifts in articles allows analysts to pinpoint early signals of technological innovation or changing consumer preferences. This capability enables businesses and investors to position themselves strategically in response to nascent trends.

In addition to identifying trends, an article scraper facilitates sentiment analysis. (Tetlock, 2007) demonstrates that media sentiment significantly influences market behaviour and investor decision-making. By aggregating and analysing textual sentiment, an article scraper provides a window into public and media perceptions of companies, sectors, and broader macroeconomic issues. These insights complement traditional financial data, offering a nuanced understanding of market drivers.

```{r, eval = FALSE}
# Ensure the directory exists
if (!dir.exists("C:/Users/jakes/Downloads")) {
    stop("Directory does not exist")
}

# Assuming 'sp500_constituents' contains a column 'Member Ticker and Exchange Code' with tickers
# Write the list of tickers to a CSV in the Downloads folder
file_path <- "C:/Users/jakes/Downloads/sp500_tickers.csv"
write.csv(sp500_constituents$`Member Ticker and Exchange Code`, 
          file_path, 
          row.names = FALSE)

# Replace "x" in the first cell (A1) with "sp500_tickers"
csv_data <- read.csv(file_path, stringsAsFactors = FALSE)
colnames(csv_data)[1] <- "sp500_tickers"
write.csv(csv_data, file_path, row.names = FALSE)

# Check if the file is written successfully
print("File written to Downloads folder with updated header.")

# Ensure the directory exists
if (!dir.exists("C:/Users/jakes/Downloads")) {
    stop("Directory does not exist")
}

# Assuming 'sp500_constituents' contains a column 'Member Ticker and Exchange Code' with tickers
# Extract only the first word (ticker) from each entry
tickers <- sapply(strsplit(sp500_constituents$`Member Ticker and Exchange Code`, " "), `[`, 1)

# Write the list of tickers to a CSV in the Downloads folder
file_path <- "C:/Users/jakes/Downloads/sp500_tickers.csv"
write.csv(data.frame(sp500_tickers = tickers), file_path, row.names = FALSE)

# Check if the file is written successfully
print("File written to Downloads folder with updated tickers.")

# Install necessary packages (only run once, or manually install them)
install.packages(c("shiny", "dplyr", "shinydashboard", "DT", "httr", "jsonlite"))

# Load necessary libraries
library(shiny)
library(dplyr)
library(shinydashboard)
library(DT)
library(httr)
library(jsonlite)

# Set up your NewsAPI Key (replace with your actual key)
news_api_key <- "cfb7ab68ee624955b00bd2a04be15f65"

# Load the list of S&P 500 tickers from a CSV
sp500_tickers <- read.csv("C:/Users/jakes/Downloads/sp500_tickers.csv", stringsAsFactors = FALSE)

# Define UI for the Shiny app
ui <- dashboardPage(
    dashboardHeader(title = "S&P500 News Scraper"),
    dashboardSidebar(
        selectInput("company", 
                    "Select Company:",
                    choices = sp500_tickers$sp500_tickers, # Dropdown for S&P 500 companies
                    selected = "AAPL"),
        actionButton("scrape_news", "Get News Articles")
    ),
    dashboardBody(
        fluidRow(
            box(title = "News Articles", width = 12, 
                DT::dataTableOutput("news_table"))
        )
    )
)

# Define server logic for the Shiny app
server <- function(input, output, session) {
    
    # Function to fetch news using NewsAPI
    fetch_news <- function(company_ticker) {
        # Validate the company_ticker
        if (is.null(company_ticker) || company_ticker == "") {
            showModal(modalDialog(
                title = "Error",
                "Please select a valid company ticker.",
                easyClose = TRUE,
                footer = modalButton("Close")
            ))
            return(NULL)
        }
        
        # Encode the ticker to ensure it's safe for URLs
        encoded_ticker <- URLencode(company_ticker)
        
        # Construct the NewsAPI endpoint URL
        url <- paste0(
            "https://newsapi.org/v2/everything?q=", encoded_ticker,
            "&from=", Sys.Date() - 30,  # Get news from the last month
            "&to=", Sys.Date(),
            "&language=en",            # Include only English results
            "&apiKey=", news_api_key
        )
        
        # Print the URL for debugging purposes
        print(paste("Fetching news from URL:", url))
        
        # Make a GET request to the NewsAPI
        response <- tryCatch({
            GET(url)
        }, error = function(e) {
            showModal(modalDialog(
                title = "Error",
                paste("Error in sending request to NewsAPI:", e$message),
                easyClose = TRUE,
                footer = modalButton("Close")
            ))
            return(NULL)
        })
        
        # Check if the request was successful
        if (is.null(response) || response$status_code != 200) {
            showModal(modalDialog(
                title = "Error",
                paste("Error in fetching news data. Status code: ", response$status_code),
                easyClose = TRUE,
                footer = modalButton("Close")
            ))
            return(NULL)
        }
        
        # Parse the response
        news_data <- tryCatch({
            fromJSON(content(response, "text", encoding = "UTF-8"))
        }, error = function(e) {
            showModal(modalDialog(
                title = "Error",
                paste("Error in parsing response from NewsAPI:", e$message),
                easyClose = TRUE,
                footer = modalButton("Close")
            ))
            return(NULL)
        })
        
        # Check if the response contains articles
        if (is.null(news_data$articles) || length(news_data$articles) == 0) {
            showModal(modalDialog(
                title = "No Articles Found",
                "No news articles found for this company.",
                easyClose = TRUE,
                footer = modalButton("Close")
            ))
            return(NULL)
        }
        
        # Prepare the data for display
        news_articles <- data.frame(
            Title = news_data$articles$title,
            Description = news_data$articles$description,
            Link = news_data$articles$url,
            PublishedAt = news_data$articles$publishedAt,
            stringsAsFactors = FALSE
        )
        
        # Return the news data
        return(news_articles)
    }
    
    # When the user clicks the button, fetch and display news
    observeEvent(input$scrape_news, {
        news_data <- fetch_news(input$company)
        
        if (!is.null(news_data)) {
            # Display the news articles in a table
            output$news_table <- DT::renderDataTable({
                DT::datatable(news_data, escape = FALSE, selection = 'single',
                              options = list(pageLength = 5,
                                             lengthMenu = c(5, 10, 25)))
            })
        }
    })
    
    # When the user clicks on a news article, open the link in the browser
    observeEvent(input$news_table_rows_selected, {
        row <- input$news_table_rows_selected
        if (length(row) > 0) {
            selected_url <- output$news_table$data[row, "Link"]
            browseURL(selected_url)
        }
    })
}

# Run the Shiny app with a fixed port
shinyApp(ui, server, options = list(port = 5000))  # Fixed port 5000, change if needed
```

## Correlation Matrix

### Reactive correlation matrix calculation
```{r, eval = FALSE}
correlation_matrix_data <- reactive({
    if (length(stock_pool$selected) < 2) {
        return(NULL)
    }

    filtered_data <- price_data_wide %>%
        filter(Date >= input$date_range[1] & Date <= input$date_range[2]) %>%
        select(c("Date", all_of(stock_pool$selected))) %>%
        mutate(across(!"Date", ~ (.x / lag(.x) - 1) * 100)) %>%
        na.omit()

    if (nrow(filtered_data) == 0) {
        return(NULL)
    }

    price_changes_matrix <- filtered_data %>%
        select(-Date)

    cor_matrix <- cor(price_changes_matrix, use = "pairwise.complete.obs")

    order <- seriate(as.dist(1 - cor_matrix), method = "HC")
    cor_matrix <- cor_matrix[get_order(order), get_order(order)]
    cor_matrix
})
```
```{r, echo=FALSE, out.width="75%"}
knitr::include_graphics("Correlation 2.png")
```
```{r, echo=FALSE}
knitr::include_graphics("Correlation 1.png")
```
The matrix calculation dynamically calculates the correlation matrix based on user-selected stocks and date range, ensuring the visualisation is always reflective of the user’s input. The clustering via hierarchical seriation enhances visual interpretability by grouping similar stocks together in the heatmap. 

```{r, eval = FALSE}
output$correlation_matrix_plot <- renderPlotly({
    cor_matrix <- correlation_matrix_data()

    cor_melted <- melt(cor_matrix, varnames = c("Var1", "Var2"))

    plot_ly(
        data = cor_melted,
        x = ~Var1, y = ~Var2, z = ~value,
        type = "heatmap",
        colors = colorRamp(c("red", "white", "blue")),
        zmin = -1,
        zmax = 1,
        hoverinfo = "x+y+z"
    ) %>%
    layout(
        title = "Correlation Matrix of Selected S&P 500 Stocks",
        xaxis = list(title = "Stocks", tickangle = 45),
        yaxis = list(title = "Stocks")
    )
})
```
The hoverinfo feature allows the user to drill down into individual correlation values for more precise insights. The colour gradient (red = negative, blue = positive) makes patterns and outliers immediately recognisable. 

### Clear All Button

```{r, eval = FALSE}
observeEvent(input$clear_all, {
    stock_pool$available <- colnames(price_data_wide)[-1]
    stock_pool$selected <- character(0)
})
```
This is a simple way to reset the stock pool and start fresh, streamlining user interaction. 

### Full code

The full code can be found below:

```{r, eval = FALSE}
library(shiny)
library(shinythemes)
library(dplyr)
library(ggplot2)
library(reshape2)
library(seriation)
library(plotly)

# Ensure `price_data_wide` exists and is valid
if (!exists("price_data_wide")) {
    stop("The dataset `price_data_wide` is missing. Please load the correct data.")
}

ui <- fluidPage(
    theme = shinytheme("flatly"),  # Apply a predefined theme
    tags$head(tags$style(HTML("
        body {
            background-color: #f9f9f9;
            font-family: Arial, sans-serif;
        }
        h1, h4 {
            color: #2c3e50;
        }
        .sidebar {
            background-color: #ecf0f1;
            border-radius: 10px;
            padding: 20px;
        }
        .sidebar .selectize-control {
            margin-bottom: 10px;
        }
        .main-panel {
            border: 2px solid #bdc3c7;
            border-radius: 10px;
            background-color: #ffffff;
            padding: 20px;
        }
        .error-message {
            color: red;
            font-weight: bold;
            font-size: 16px;
        }
    "))),
    titlePanel("S&P 500 Correlation Matrix"),
    sidebarLayout(
        sidebarPanel(
            class = "sidebar",
            h4("Correlation Matrix of S&P 500 Constituents"),
            helpText("Select stocks and date range for the heatmap."),
            dateRangeInput(
                "date_range",
                "Choose Date Range:",
                start = min(price_data_wide$Date),
                end = max(price_data_wide$Date),
                min = min(price_data_wide$Date),
                max = max(price_data_wide$Date)
            ),
            uiOutput("available_stocks_ui"),  # Dynamic UI for available stocks
            uiOutput("selected_stocks_ui"),  # Dynamic UI for selected stocks
            actionButton("clear_all", "Clear All Stocks", class = "btn btn-danger")  # Clear all button
        ),
        mainPanel(
            class = "main-panel",
            uiOutput("plot_or_message")  # Dynamic output: Plot or message
        )
    )
)

server <- function(input, output, session) {
    # Reactive values to manage the stock pool
    stock_pool <- reactiveValues(
        available = colnames(price_data_wide)[-1], # Initial stock pool (excluding Date)
        selected = colnames(price_data_wide)[-1]  # Select all stocks by default
    )

    # Update UI for available stocks
    output$available_stocks_ui <- renderUI({
        selectInput(
            "available_stocks",
            "Available stocks:",
            choices = stock_pool$available,
            selected = NULL,
            multiple = TRUE
        )
    })

    # Update UI for selected stocks
    output$selected_stocks_ui <- renderUI({
        selectInput(
            "selected_stocks",
            "Selected stocks:",
            choices = stock_pool$selected,
            selected = stock_pool$selected,
            multiple = TRUE
        )
    })

    # Observe selection from available stocks to add
    observeEvent(input$available_stocks, {
        selected_to_add <- input$available_stocks
        stock_pool$selected <- unique(c(stock_pool$selected, selected_to_add))
        stock_pool$available <- setdiff(stock_pool$available, selected_to_add)
    })

    # Observe selection from selected stocks to remove
    observeEvent(input$selected_stocks, {
        selected_to_remove <- setdiff(stock_pool$selected, input$selected_stocks)
        stock_pool$available <- unique(c(stock_pool$available, selected_to_remove))
        stock_pool$selected <- setdiff(stock_pool$selected, selected_to_remove)
    })

    # Clear all stocks when the "Clear All" button is clicked
    observeEvent(input$clear_all, {
        stock_pool$available <- colnames(price_data_wide)[-1]  # Reset all stocks to available
        stock_pool$selected <- character(0)  # Clear selected stocks
    })

    # Reactive correlation matrix calculation
    correlation_matrix_data <- reactive({
        # Ensure at least two stocks are selected
        if (length(stock_pool$selected) < 2) {
            return(NULL)
        }

        # Filter data by date range and selected stocks
        filtered_data <- price_data_wide %>%
            filter(Date >= input$date_range[1] & Date <= input$date_range[2]) %>%
            select(c("Date", all_of(stock_pool$selected))) %>%
            mutate(across(!"Date", ~ (.x / lag(.x) - 1) * 100)) %>%
            na.omit()

        # If filtered data is empty, return NULL
        if (nrow(filtered_data) == 0) {
            return(NULL)
        }

        # Drop the Date column for correlation matrix calculation
        price_changes_matrix <- filtered_data %>%
            select(-Date)

        # Compute the correlation matrix (ranges between -1 and 1 naturally)
        cor_matrix <- cor(price_changes_matrix, use = "pairwise.complete.obs")

        # Cluster rows and columns
        order <- seriate(as.dist(1 - cor_matrix), method = "HC")
        cor_matrix <- cor_matrix[get_order(order), get_order(order)]
        cor_matrix
    })

    # Dynamic output: Either plot or message
    output$plot_or_message <- renderUI({
        cor_matrix <- correlation_matrix_data()

        if (is.null(cor_matrix)) {
            return(
                div(
                    class = "error-message",
                    "Error: No data available for the selected date range and stocks."
                )
            )
        }

        plotlyOutput("correlation_matrix_plot")
    })

    # Render the interactive heatmap
    output$correlation_matrix_plot <- renderPlotly({
        cor_matrix <- correlation_matrix_data()

        # Melt the correlation matrix for plotly
        cor_melted <- melt(cor_matrix, varnames = c("Var1", "Var2"))

        # Generate interactive heatmap
        plot_ly(
            data = cor_melted,
            x = ~Var1, y = ~Var2, z = ~value,
            type = "heatmap",
            colors = colorRamp(c("red", "white", "blue")),
            zmin = -1,  # Set fixed scale minimum
            zmax = 1,   # Set fixed scale maximum
            hoverinfo = "x+y+z"
        ) %>%
        layout(
            title = "Correlation Matrix of Selected S&P 500 Stocks",
            xaxis = list(title = "Stocks", tickangle = 45),
            yaxis = list(title = "Stocks")
        )
    })
}

# Run the application
shinyApp(ui = ui, server = server)
```
## Bollinger Bands Visualisation tool

Here is the full code for the visualisation tool: 

```{r, eval = FALSE}
# Load necessary libraries
library(shiny)
library(plotly)
library(DT)
library(bslib)

# UI Definition
ui <- fluidPage(
    theme = bs_theme(
        bootswatch = "flatly",
        primary = "#0056b3",
        secondary = "#FF5733"
    ),
    
    titlePanel(
        tags$h1("Interactive Bollinger Bands Simulation", style = "text-align: center; color: #0056b3;")
    ),
    
    sidebarLayout(
        sidebarPanel(
            tags$h4("Configuration", style = "color: #FF5733;"),
            selectizeInput(
                inputId = "ticker",
                label = "Select Ticker:",
                choices = NULL,
                options = list(
                    placeholder = "Search or select a ticker...",
                    maxOptions = 503
                )
            ),
            dateRangeInput("date_range", "Select Date Range:", start = "2010-01-01", end = Sys.Date()),
            numericInput("initial_capital", "Initial Capital (£):", value = 1e7, min = 1e4, step = 1e6),
            numericInput("transaction_cost", "Transaction Cost (%):", value = 0, min = 0, max = 100, step = 0.1),
            numericInput("position_size", "Position Size (% of capital):", value = 10, min = 1, max = 100, step = 1),
            actionButton("update", "Update", class = "btn-primary btn-lg"),
            width = 3
        ),
        
        mainPanel(
            tabsetPanel(
                tabPanel(
                    "Bar Chart",
                    tags$h3("Signal Proportions (BUY/SELL)", style = "text-align: center;"),
                    plotlyOutput("bar_chart", height = "400px")
                ),
                tabPanel(
                    "Scatter Plot",
                    tags$h3("Price with Bollinger Bands and Breaches", style = "text-align: center;"),
                    plotlyOutput("scatter_plot", height = "400px")
                ),
                tabPanel(
                    "Line Plot",
                    tags$h3("Cumulative Profit (£)", style = "text-align: center;"),
                    plotlyOutput("line_plot", height = "400px")
                ),
                tabPanel(
                    "Signal Table",
                    tags$h3("Detailed Signal Data", style = "text-align: center;"),
                    DTOutput("signals_table")
                )
            )
        )
    )
)

# Server Definition
server <- function(input, output, session) {
    # Load tickers directly from ticker_full (character vector)
    observe({
        req(ticker_full)
        if (!is.character(ticker_full)) {
            showNotification("ticker_full should be a character vector.", type = "error")
            return()
        }
        
        # Update the ticker dropdown
        updateSelectizeInput(session, "ticker", choices = ticker_full, server = TRUE)
    })
    
    # Reactive data based on user inputs
    filtered_data <- reactive({
        req(input$ticker, input$date_range, signals_list)
        
        # Fetch data for the selected ticker
        data <- signals_list[[input$ticker]]
        
        # Check if the dataset is empty
        if (is.null(data) || nrow(data) == 0) {
            showNotification("The selected ticker data is empty or unavailable.", type = "error")
            return(NULL)
        }
        
        # Generate Date column if missing
        if (!"Date" %in% names(data)) {
            data$Date <- seq.Date(from = as.Date("2010-01-01"), by = "day", length.out = nrow(data))
        }
        
        # Convert the Date column to Date type
        data$Date <- as.Date(data$Date, format = "%Y-%m-%d")
        
        # Filter data by date range
        data <- subset(data, Date >= input$date_range[1] & Date <= input$date_range[2])
        
        # Remove rows with missing data
        data <- na.omit(data)
        
        if (nrow(data) < 1) {
            showNotification("No data available for the selected date range.", type = "error")
            return(NULL)
        }
        
        return(data)
    })
    
    # Calculate Cumulative Profit Iteratively
    cumulative_profit <- reactive({
        req(filtered_data())
        data <- filtered_data()
        initial_capital <- input$initial_capital
        transaction_cost <- input$transaction_cost / 100
        position_size <- input$position_size / 100
        
        # Add a new column for cumulative profit
        data$CumulativeProfit <- 0
        capital <- initial_capital
        
        for (i in seq_len(nrow(data))) {
            if (data$Signal[i] == "BUY") {
                # Calculate the cost of the trade
                trade_value <- capital * position_size
                cost <- trade_value * transaction_cost
                capital <- capital - cost
            } else if (data$Signal[i] == "SELL") {
                # Simulate a profit/loss on the trade
                if (i > 1) {
                    trade_value <- capital * position_size
                    cost <- trade_value * transaction_cost
                    profit <- (data$Price[i] - data$Price[i - 1]) * trade_value / data$Price[i - 1]
                    capital <- capital + profit - cost
                }
            }
            # Update cumulative profit
            data$CumulativeProfit[i] <- capital - initial_capital
        }
        
        return(data)
    })
    
    # Bar Chart for Signal Proportions
    output$bar_chart <- renderPlotly({
        req(filtered_data())
        data <- filtered_data()
        signal_counts <- table(data$Signal[data$Signal %in% c("BUY", "SELL")])
        
        plot_ly(
            x = names(signal_counts),
            y = as.numeric(signal_counts),
            type = "bar",
            marker = list(color = c("#1abc9c", "#e74c3c"))
        ) %>% layout(
            title = paste("Signal Proportions (BUY/SELL) for", input$ticker),
            xaxis = list(title = "Signal"),
            yaxis = list(title = "Frequency")
        )
    })
    
    # Scatter Plot with Bollinger Bands
    output$scatter_plot <- renderPlotly({
        req(filtered_data())
        data <- filtered_data()
        
        plot_ly(data, x = ~Date) %>%
            add_lines(y = ~Price, name = "Price", line = list(color = "blue")) %>%
            add_lines(y = ~BB_Upper, name = "BB Upper", line = list(color = "red", dash = "dash")) %>%
            add_lines(y = ~BB_Lower, name = "BB Lower", line = list(color = "green", dash = "dash")) %>%
            add_markers(data = subset(data, Signal == "BUY"), x = ~Date, y = ~Price, name = "BUY", marker = list(color = "green", size = 8)) %>%
            add_markers(data = subset(data, Signal == "SELL"), x = ~Date, y = ~Price, name = "SELL", marker = list(color = "red", size = 8)) %>%
            layout(
                title = paste("Bollinger Bands for", input$ticker),
                xaxis = list(title = "Date"),
                yaxis = list(title = "Price")
            )
    })
    
    # Line Plot for Cumulative Profit
    output$line_plot <- renderPlotly({
        req(cumulative_profit())
        data <- cumulative_profit()
        
        plot_ly(data, x = ~Date, y = ~CumulativeProfit, type = "scatter", mode = "lines") %>%
            layout(
                title = paste("Cumulative Profit (£) for", input$ticker),
                xaxis = list(title = "Date"),
                yaxis = list(title = "Cumulative Profit (£)")
            )
    })
    
    # Signal Table
    output$signals_table <- renderDT({
        req(filtered_data())
        datatable(filtered_data(), options = list(pageLength = 10, autoWidth = TRUE))
    })
}

# Run the application
shinyApp(ui = ui, server = server)
```

## AAPL Financial Statements

Please see the full code here: 

```{r, eval = FALSE}
# Install and load required packages
if (!require("httr")) install.packages("httr")
if (!require("jsonlite")) install.packages("jsonlite")
if (!require("openxlsx")) install.packages("openxlsx")

library(httr)
library(jsonlite)
library(openxlsx)

# Set your Alpha Vantage API key
api_key <- "8NIAD65MWQFINSTH"
symbol <- "AAPL"

# Define API URLs
url_income <- paste0("https://www.alphavantage.co/query?function=INCOME_STATEMENT&symbol=", symbol, "&apikey=", api_key)
url_balance <- paste0("https://www.alphavantage.co/query?function=BALANCE_SHEET&symbol=", symbol, "&apikey=", api_key)
url_cash_flow <- paste0("https://www.alphavantage.co/query?function=CASH_FLOW&symbol=", symbol, "&apikey=", api_key)

# Fetch data
income_statement <- fromJSON(content(GET(url_income), as = "text"))
balance_sheet <- fromJSON(content(GET(url_balance), as = "text"))
cash_flow <- fromJSON(content(GET(url_cash_flow), as = "text"))

# Extract relevant data from API responses
income_statement_data <- as.data.frame(income_statement$annualReports)
balance_sheet_data <- as.data.frame(balance_sheet$annualReports)
cash_flow_data <- as.data.frame(cash_flow$annualReports)

# Create Excel workbook
wb <- createWorkbook()

# Add sheets and write data
addWorksheet(wb, "Income Statement")
writeData(wb, "Income Statement", income_statement_data)

addWorksheet(wb, "Balance Sheet")
writeData(wb, "Balance Sheet", balance_sheet_data)

addWorksheet(wb, "Cash Flow Statement")
writeData(wb, "Cash Flow Statement", cash_flow_data)

# Define the file path for Downloads folder
downloads_path <- file.path(Sys.getenv("USERPROFILE"), "Downloads", "financial_statements.xlsx")

# Save the Excel file in the Downloads folder
saveWorkbook(wb, file = downloads_path, overwrite = TRUE)

cat("Financial statements saved to your Downloads folder as 'financial_statements.xlsx'.\n")

# Install and load required packages
if (!require("readxl")) install.packages("readxl")
if (!require("dplyr")) install.packages("dplyr")

library(readxl)
library(dplyr)

# Define the file path
file_path <- "financial_statements.xlsx"

# Sheet names that correspond to the statements
sheet_income <- "Income Statement"
sheet_balance <- "Balance Sheet"
sheet_cashflow <- "Cash Flow Statement"

# Function to clean and extract the last 3 years
clean_statement <- function(sheet_name, file_path) {
  # Read the sheet
  data <- read_excel(file_path, sheet = sheet_name)
  
  # Check if the fiscalDateEnding column exists and clean the data
  if ("fiscalDateEnding" %in% colnames(data)) {
    cleaned_data <- data %>%
      mutate(fiscalDateEnding = as.Date(fiscalDateEnding)) %>%  # Convert to Date
      arrange(desc(fiscalDateEnding)) %>%  # Sort by date
      slice(1:3)  # Select the last 3 years
    return(cleaned_data)
  } else {
    stop(paste("fiscalDateEnding column not found in", sheet_name))
  }
}

# Clean each statement
income_statement_cleaned <- clean_statement(sheet_income, file_path)
balance_sheet_cleaned <- clean_statement(sheet_balance, file_path)
cash_flow_cleaned <- clean_statement(sheet_cashflow, file_path)

# Display the cleaned tables
print("Income Statement (Last 3 Years):")
print(income_statement_cleaned)

print("Balance Sheet (Last 3 Years):")
print(balance_sheet_cleaned)

print("Cash Flow Statement (Last 3 Years):")
print(cash_flow_cleaned)
```
## Evaluating model performance - Extended risk analysis

Firstly, the VaR Sharpe explores the potential for extreme losses by measuring the worst-case scenario at a given confidence level. The VaR Sharpe value is .26, suggesting that over a 1-day horizon, the model is able to produce returns that more than compensate for its extreme risk. This is a favorable outcome, indicating that the model’s worst-case scenario (tail risk) is managed effectively in the short term. However, in both the 5 and 10 day model, the models have VaR values of -.15 and -.26 respectively, the model is exposed to greater extreme risks that are not compensated by sufficient returns, indicating underperformance.

Whilst VaR provides the maximum potential loss at a given confidence level, Expected Shortfall Sharpe (Conditional VaR) quantifies the average loss in the tail of the distribution beyond the VaR threshold - it gives the expected value of losses if the loss exceeds the VaR threshold. The positive value in the 1-day model suggests the model is generating sufficient returns to offset the risk of extreme losses, making it well-adjusted for downside risk, whereas with both the 5 and 10 day model, they are expected to suffer substantial average losses in the worst-case scenarios, thereby not compensated by returns. 

Finally, SemiSD Sharpe considers only downside risk (negative volatility), providing a more relevant metric for evaluating risk from the perspective of loss aversion. The 1-day model has a semiSD Sharpe value of .31, showing that the model offers solid returns for the downside risk it takes on. For the 5-day model, the sign changes, and the coefficient becomes -.27; the model incurs considerable downside risk that is not compensated by sufficient returns, signaling poor performance over the medium-term horizon. The coefficient decreases to -.56 in the 10-day model, suggesting even worse performance. 

## Full code - Random Forest Bollinger Bands regression model

```{r, eval = FALSE}
# Load necessary libraries
library(randomForest)
library(tidyverse)
library(lubridate)
library(doParallel)
library(iml)        # For SHAP, PDP, ICE
library(forecast)   # For autocorrelation diagnostics
library(PerformanceAnalytics)  # For Sharpe ratio
library(zoo)        # For forward filling NA values
library(openxlsx)   # For Excel writing

# Step 1: Extract AAPL data from signals_list
cat("Step 1: Extracting AAPL data...\n")
tryCatch({
    aapl_data <- signals_list[["AAPL US Equity"]]
}, error = function(e) {
    stop("Error in Step 1: Failed to extract AAPL data.\n", e$message)
})

# Step 2: Ensure Date is in Date format and data is sorted
cat("Step 2: Formatting and sorting data...\n")
tryCatch({
    aapl_data <- aapl_data %>% 
        mutate(Date = as.Date(Date)) %>% 
        arrange(Date)
}, error = function(e) {
    stop("Error in Step 2: Formatting and sorting failed.\n", e$message)
})

# Step 3: Add external predictors dynamically (only VIX)
cat("Step 3: Adding VIX data...\n")
tryCatch({
    if (!exists("vix_data")) {
        stop("vix_data is not loaded. Please load the VIX data into the environment before running the script.")
    }
    
    # Rename 'VIX.Adjusted' to 'VIX' if it exists
    if ("VIX.Adjusted" %in% colnames(vix_data)) {
        colnames(vix_data)[colnames(vix_data) == "VIX.Adjusted"] <- "VIX"
    }
    
    # Convert 'Date' to Date type if necessary
    vix_data$Date <- as.Date(vix_data$Date)
    
    # Remove rows with missing VIX values
    vix_data <- vix_data %>% filter(!is.na(VIX))
    
    # Merge VIX data with AAPL data
    cat("Merging VIX data with aapl_data...\n")
    aapl_data <- aapl_data %>% left_join(vix_data, by = "Date")
}, error = function(e) {
    stop("Error in Step 3: Adding external predictors failed.\n", e$message)
})

# Step 4: Add required features dynamically
cat("Step 4: Adding features dynamically...\n")
tryCatch({
    # Forward fill missing values in Bollinger Bands
    aapl_data <- aapl_data %>% 
        mutate(
            BB_Upper = na.locf(BB_Upper, na.rm = FALSE),
            BB_Lower = na.locf(BB_Lower, na.rm = FALSE)
        )
    
    # Replace remaining NA values with 0
    aapl_data <- aapl_data %>% 
        mutate(
            BB_Upper = ifelse(is.na(BB_Upper), 0, BB_Upper),
            BB_Lower = ifelse(is.na(BB_Lower), 0, BB_Lower)
        )
    
    # Add dynamic features
    aapl_data <- aapl_data %>% 
        mutate(
            Band_Width = BB_Upper - BB_Lower,
            Proximity_to_Upper = BB_Upper - Price,
            Proximity_to_Lower = Price - BB_Lower,
            Band_Width_Lag1 = lag(Band_Width, 1),
            Z_Statistic_Lag1 = lag(Z_Statistic, 1),
            Interaction_BW_VIX = Band_Width * VIX,
            return_1d = (lead(Price, 1) - Price) / Price,
            return_5d = (lead(Price, 5) - Price) / Price,
            return_10d = (lead(Price, 10) - Price) / Price
        ) %>% 
        drop_na(Band_Width, Proximity_to_Upper, Proximity_to_Lower, 
                Band_Width_Lag1, Z_Statistic_Lag1, return_1d, return_5d, return_10d)
}, error = function(e) {
    stop("Error in Step 4: Feature engineering failed.\n", e$message)
})

# Step 5: Split the dataset using time-series validation
cat("Step 5: Splitting dataset into training and testing sets...\n")
tryCatch({
    cut_off_date <- "2020-01-01"  # Define split date
    train_data <- aapl_data %>% filter(Date < as.Date(cut_off_date))
    test_data <- aapl_data %>% filter(Date >= as.Date(cut_off_date))
}, error = function(e) {
    stop("Error in Step 5: Data splitting failed.\n", e$message)
})

# Step 6: Set Date column as rownames
cat("Step 6: Setting Date as rownames for train and test data...\n")
train_data <- train_data %>% column_to_rownames(var = "Date")
test_data <- test_data %>% column_to_rownames(var = "Date")

# Step 7: Set up parallelisation
cat("Step 7: Setting up parallelisation...\n")
tryCatch({
    cl <- makeCluster(detectCores() - 1)
    registerDoParallel(cl)
}, error = function(e) {
    stop("Error in Step 7: Parallelisation setup failed.\n", e$message)
})

# Step 8: Define evaluation metrics
evaluate_metrics <- function(predictions, actuals, threshold = 0) {
    directional_accuracy <- mean(sign(predictions) == sign(actuals))
    hit_rate <- mean(predictions > threshold & actuals > threshold)
    sharpe_ratio <- SharpeRatio(predictions - actuals, Rf = 0.02 / 252, p = 0.95)
    
    return(data.frame(
        Directional_Accuracy = directional_accuracy,
        Hit_Rate = hit_rate,
        Sharpe_Ratio = sharpe_ratio
    ))
}

# Step 9: Hyperparameter Tuning with Grid Search using tuneRF for 1-day, 5-day, and 10-day models
cat("Step 9: Hyperparameter tuning with tuneRF for 1-day, 5-day, and 10-day...\n")

# Hyperparameter tuning for 1-day model
tune_1d <- tuneRF(
  x = train_data[, c("Band_Width", "Proximity_to_Upper", "Proximity_to_Lower", "VIX", 
                     "Band_Width_Lag1", "Z_Statistic_Lag1", "Interaction_BW_VIX")],
  y = train_data$return_1d,                    
  mtryStart = 3,                                
  ntreeTry = 500,                               
  stepFactor = 1.5,                             
  improve = 0.01,                              
  trace = TRUE,                                 
  plot = FALSE                                   
)

# Hyperparameter tuning for 5-day model
tune_5d <- tuneRF(
  x = train_data[, c("Band_Width", "Proximity_to_Upper", "Proximity_to_Lower", "VIX", 
                     "Band_Width_Lag1", "Z_Statistic_Lag1", "Interaction_BW_VIX")],
  y = train_data$return_5d,                    
  mtryStart = 3,                                
  ntreeTry = 500,                               
  stepFactor = 1.5,                             
  improve = 0.01,                              
  trace = TRUE,                                 
  plot = FALSE                                   
)

# Hyperparameter tuning for 10-day model
tune_10d <- tuneRF(
  x = train_data[, c("Band_Width", "Proximity_to_Upper", "Proximity_to_Lower", "VIX", 
                     "Band_Width_Lag1", "Z_Statistic_Lag1", "Interaction_BW_VIX")],
  y = train_data$return_10d,                    
  mtryStart = 3,                                
  ntreeTry = 500,                               
  stepFactor = 1.5,                             
  improve = 0.01,                              
  trace = TRUE,                                 
  plot = FALSE                                   
)

# Step 10: Train final models for each prediction horizon
final_rf_model_1d <- randomForest(
    return_1d ~ Band_Width + Proximity_to_Upper + Proximity_to_Lower + VIX +
        Band_Width_Lag1 + Z_Statistic_Lag1 + Interaction_BW_VIX,
    data = train_data,
    ntree = 500,
    mtry = tune_1d[1, "mtry"],
    importance = TRUE
)

final_rf_model_5d <- randomForest(
    return_5d ~ Band_Width + Proximity_to_Upper + Proximity_to_Lower + VIX +
        Band_Width_Lag1 + Z_Statistic_Lag1 + Interaction_BW_VIX,
    data = train_data,
    ntree = 500,
    mtry = tune_5d[1, "mtry"],
    importance = TRUE
)

final_rf_model_10d <- randomForest(
    return_10d ~ Band_Width + Proximity_to_Upper + Proximity_to_Lower + VIX +
        Band_Width_Lag1 + Z_Statistic_Lag1 + Interaction_BW_VIX,
    data = train_data,
    ntree = 500,
    mtry = tune_10d[1, "mtry"],
    importance = TRUE
)

# Step 11: Evaluate models and save performance metrics
metrics_1d <- evaluate_metrics(predict(final_rf_model_1d, test_data), test_data$return_1d)
metrics_5d <- evaluate_metrics(predict(final_rf_model_5d, test_data), test_data$return_5d)
metrics_10d <- evaluate_metrics(predict(final_rf_model_10d, test_data), test_data$return_10d)

# Combine metrics into a dataframe
model_metrics <- bind_rows(
  data.frame(Model = "1-day", metrics_1d),
  data.frame(Model = "5-day", metrics_5d),
  data.frame(Model = "10-day", metrics_10d)
)

# Save metrics as CSV
write.csv(model_metrics, "model_performance_metrics.csv", row.names = FALSE)

# Step 12: Save Actual vs Predicted Plots
png("predictions_vs_actuals_1d.png", width = 3000, height = 3000)
plot(predict(final_rf_model_1d, test_data), test_data$return_1d, 
     main = "1-Day Predictions vs Actuals", 
     xlab = "Predicted Returns", ylab = "Actual Returns", col = "blue", pch = 16)
abline(0, 1, col = "red")
dev.off()

png("predictions_vs_actuals_5d.png", width = 3000, height = 3000)
plot(predict(final_rf_model_5d, test_data), test_data$return_5d, 
     main = "5-Day Predictions vs Actuals", 
     xlab = "Predicted Returns", ylab = "Actual Returns", col = "blue", pch = 16)
abline(0, 1, col = "red")
dev.off()

png("predictions_vs_actuals_10d.png", width = 3000, height = 3000)
plot(predict(final_rf_model_10d, test_data), test_data$return_10d, 
     main = "10-Day Predictions vs Actuals", 
     xlab = "Predicted Returns", ylab = "Actual Returns", col = "blue", pch = 16)
abline(0, 1, col = "red")
dev.off()

# Step 13: Save performance metrics in Excel
write.xlsx(list("1-day" = metrics_1d, "5-day" = metrics_5d, "10-day" = metrics_10d, "Summary" = model_metrics), 
           "model_performance_metrics.xlsx")

# Stop the parallel cluster
stopCluster(cl)
```

### SHAP, ALE, ICE full code

```{r, eval = FALSE}
# Load necessary libraries
library(randomForest)
library(tidyverse)
library(lubridate)
library(iml)         # For SHAP, ALE, ICE
library(ggplot2)      # For plotting
library(openxlsx)     # For saving results

# Assuming that your models and data are already loaded
# final_rf_model_1d, final_rf_model_5d, final_rf_model_10d, and aapl_data

# Select relevant features for SHAP, ALE, and ICE analysis
feature_cols <- c("Band_Width", "Proximity_to_Upper", "Proximity_to_Lower", 
                  "VIX", "Band_Width_Lag1", "Z_Statistic_Lag1", "Interaction_BW_VIX")

# Create a DataFrame with the features used in the model
model_data <- aapl_data %>% select(all_of(feature_cols))

# Step 1: SHAP (SHapley Additive exPlanations) for Feature Importance
cat("Step 1: Calculating SHAP values...\n")

# Create a predictor object for the 1-day model (Repeat for 5-day and 10-day models as necessary)
predictor_1d <- Predictor$new(final_rf_model_1d, data = model_data, y = aapl_data$return_1d)

# SHAP values for the 1-day model
shap_1d <- Shapley$new(predictor_1d, x.interest = model_data[1, , drop = FALSE])  # Select an instance (first row here)

# Plot SHAP values for the 1-day model
shap_1d$plot()

# Step 2: ALE (Partial Dependence Plot) for Features
cat("Step 2: Plotting Partial Dependence Plots...\n")

# ALE for Band_Width feature (Change to other features as needed)
pdp_bandwidth <- FeatureEffect$new(predictor_1d, feature = "Band_Width")
pdp_bandwidth$plot()

# Step 3: ICE (Individual Conditional Expectation) for Features
cat("Step 3: Plotting ICE (Individual Conditional Expectation)...\n")

# ICE plot for Band_Width feature (You can change the feature for other ICE plots)
ice_bandwidth <- FeatureEffect$new(predictor_1d, feature = "Band_Width", method = "ice")
ice_bandwidth$plot()

# Repeat steps for 5-day and 10-day models

# For 5-day model
predictor_5d <- Predictor$new(final_rf_model_5d, data = model_data, y = aapl_data$return_5d)
shap_5d <- Shapley$new(predictor_5d, x.interest = model_data[1, , drop = FALSE])
shap_5d$plot()

pdp_bandwidth_5d <- FeatureEffect$new(predictor_5d, feature = "Band_Width")
pdp_bandwidth_5d$plot()

ice_bandwidth_5d <- FeatureEffect$new(predictor_5d, feature = "Band_Width", method = "ice")
ice_bandwidth_5d$plot()

# For 10-day model
predictor_10d <- Predictor$new(final_rf_model_10d, data = model_data, y = aapl_data$return_10d)
shap_10d <- Shapley$new(predictor_10d, x.interest = model_data[1, , drop = FALSE])
shap_10d$plot()

pdp_bandwidth_10d <- FeatureEffect$new(predictor_10d, feature = "Band_Width")
pdp_bandwidth_10d$plot()

ice_bandwidth_10d <- FeatureEffect$new(predictor_10d, feature = "Band_Width", method = "ice")
ice_bandwidth_10d$plot()

# Step 4: Save Results to Excel
cat("Step 4: Saving results to Excel...\n")

# Collect SHAP values and other results
shap_values_1d <- shap_1d$results
shap_values_5d <- shap_5d$results
shap_values_10d <- shap_10d$results

# Save SHAP, PDP, and ICE results to an Excel file
write.xlsx(list(
    "SHAP_1D" = shap_values_1d,
    "SHAP_5D" = shap_values_5d,
    "SHAP_10D" = shap_values_10d,
    "PDP_1D" = pdp_bandwidth$results,
    "PDP_5D" = pdp_bandwidth_5d$results,
    "PDP_10D" = pdp_bandwidth_10d$results,
    "ICE_1D" = ice_bandwidth$results,
    "ICE_5D" = ice_bandwidth_5d$results,
    "ICE_10D" = ice_bandwidth_10d$results
), "shap_pdp_ice_results.xlsx")

cat("Results saved to 'shap_pdp_ice_results.xlsx'.\n")

# Step 5: Close parallel cluster if it's still running
stopCluster(cl)
```



## Reflection

Regarding the position size within the cumulative profit simulation, I could've allowed for heterogeneity between buy and sell trades. To further add interactivity and customisability, I could'allowed the user to backtest an entire portfolio rather than a singular stock. 

From an evaluative perspective, I could've definitely added more value through including interaction terms in the independent variables between other indicators like RSI or MACD. This would've compared the pure Bollinger Bands strategy with merged strategies, and computed the difference in trading efficacy between the strategies. 

During the coding project, I dedicated considerable time to developing and refining my code, but I recognised areas where my approach could have been more efficient. For example, the proliferation of variables occasionally complicated the structure, and certain sections, such as the repeated importation of libraries, could have been consolidated to improve overall functionality 

While leveraging GitHub for version control was invaluable, I underutilised its potential as a resource for reviewing and informing the logical progression of my work. Upon reflection, adopting a more methodical approach to variable management, actively refactoring code for greater efficiency, and systematically consulting prior iterations could have enhanced both the clarity and effectiveness of my project.

I occasionally found myself losing sight of the overall vision. I had a tendency to prioritise building features that I found personally enjoyable, often retrofitting them into the project later rather than ensuring they aligned with the initial objectives. While this approach allowed me to explore creative ideas, it sometimes diverted focus from the project's core purpose. That said, I greatly enjoyed the coding process itself, particularly the problem-solving aspect of debugging, which was both intellectually stimulating and rewarding. Reflecting on this, I see value in balancing creative exploration with a clearer adherence to the project's overarching goals.

Regarding the Random Forest model, I didn't test for multicollinearity between Proximity_to_Upper and Proximity_to_Lower. These features may contain redundant information, which could negatively impact the model's ability to generalise. In future, I'd create a correlation matrix explicitly for model variables, or conduct a multicollinearity check before deciding on my final independent variables. Whilst I used hyperparameter tuning, I think I could've attempted more specific checks such as cross-validation to enhance model robustness. 

When considering the different measures of risk-adjusted returns, I realised I didn't clearly consider or conduct sensitivity analysis on an appropriate confidence level, instead opting for the default (95%) confidence level. In future, running models at the 90%, 95% and 99% confidence level could generate a further level of comments, as coefficients change.

To maximise frugality of the model, I could've highlighted individual coefficients better. I found it challenging, given the Random Forest model doesn't have directly interpretable coefficients. Additionally, when considering the lagged variables, I could've justified using Akaike Information Criterion or Bayesian Information Criterion to determine optimal leg length opposed to lagging the band width to 1.

When portraying the SHAP, ALE, and ICE values, I could've put the values into data tables (either in the main body or the Appendix) to give additional context to the reader. Due to the constrained wordcount, I only highlighted the most important phenomena.

Finally, to determine the efficacy of the Random Forest Bollinger Bands model, I would include a baseline regression model in the future to be able to discriminate and directly compare between the models. 
