---
title: "."
author: "Jake Smith"
date: "`r Sys.Date()`"
output:
  word_document:
    toc: true              # Enables the table of contents
    toc_depth: 3           # Depth of headings included in the ToC
    number_sections: true  # Numbers the sections
---
The use of Bollinger Bands in financial analysis has gained significant traction as a robust tool for understanding price volatility and identifying trading opportunities. Bollinger Bands, a technical indicator developed by John Bollinger, consist of a moving average with upper and lower bands that adapt dynamically to market volatility(Bollinger, 2001).These bands provide traders with a visual framework to identify periods of relative price strength or weakness, as well as potential breakout or reversion points. In particular, mean reversion strategies, which assume that asset prices tend to revert to their historical average, are often employed alongside Bollinger Bands to capitalise on price fluctuations and market inefficiencies.

# Introduction

The importance of such strategies lies in their ability to provide actionable insights in a wide range of market conditions. As markets become increasingly influenced by algorithmic trading and high-frequency strategies, understanding price behaviour through indicators like Bollinger Bands becomes essential for identifying profitable opportunities while mitigating risk. Mean reversion strategies complement this by focusing on statistical anomalies, enabling traders and portfolio managers to make data-driven decisions based on the likelihood of price reversion (Lo et al., 2007).

This project aims to explore the relationship between Bollinger Bands and mean reversion strategies, assessing their effectiveness in different market environments. By employing visualisation techniques, I aim to show the user how Bollinger Bands act as a foundation to more complex mean reversion strategies. The insights gained from this study will not only enhance understanding of these strategies but also provide a foundation for developing more sophisticated trading models and risk management frameworks.

As an ancillary objective, this project serves as an interactive financial visualisation tool, aiming to bridge the academia-reality bridge. Some of the capabilities of the Shiny UI include an article scraper, examining causality between either the absolute or logarithmic prices of two stocks, and an article summariser, aiming to reduce the manual intensity of equity research when searching for a stock. Full code will be provided throughout the bulk of the article, and in various appendix sections. 

# Literature Review

A Bollinger Band is a technical analysis tool composed of three lines. The middle band is a simple moving average of the security’s price over a specified period (In our example, this is 20 days). The upper band is two standard deviations above the middle band, representing a dynamic resistance level, whereas the lower band lies at two standard deviations below the middle band, representing a dynamic support level (Bollinger, 1992). 

The mathematical derivation is as follows: 
For any time t, the Bollinger Bands are:


$$
Middle\ Band\ (SMA) = \frac{1}{n} \sum_{i=t-n+1}^{t} P_i
$$

Standard Deviation: 

$$
\sigma_t = \sqrt{\frac{1}{n} \sum_{i=t-n+1}^{t} \left( P_i - \frac{1}{n} \sum_{j=t-n+1}^{t} P_j \right)^2}
$$


Note that $P_i - \frac{1}{n}$ is subtracted rather than added as standard deviation measures the distance data points deviate from the mean. Since deviations can be negative or positive, squaring ensures all deviations contribute positively to the variance.

The lower and upper bands are expressed as:

$$
\text{Lower Band} = \frac{1}{n} \sum_{i=t-n+1}^{t} P_i - k \sqrt{\frac{1}{n} \sum_{i=t-n+1}^{t} \left(P_i - \text{SMA}_n\right)^2}
$$

$$
\text{Upper Band} = \frac{1}{n} \sum_{i=t-n+1}^{t} P_i + k \sqrt{\frac{1}{n} \sum_{i=t-n+1}^{t} \left(P_i - \text{SMA}_n\right)^2}
$$

## Applications in Financial Markets

The Bollinger Bands mean reversion strategy is widely recognised, yet consensus around the most optimal application hasn’t yet converged. (Chen, Et al, 2018) explores a Bollinger Bands trading strategy based on wavelet analysis, considering returns, drawdowns, and the income-risk ratio. Using the CSI 300 stock index futures as the research subject, they applied wavelet noise reduction to the price data and found that the enhanced Bollinger Bands trading technique offered higher returns and lower risk compared with conventional BB. For more information on the derivation of wavelet noise reduction, please see the Appendix. (Yan, Et al, 2023) analysed returns in the HKSE based on Random Forest and Bollinger Bands. The performance of traditional BB and enhanced BB strategy are compared and concluded that a combination of traditional and enhanced BB generates higher returns than simply investing in stocks. More information around Random Forest can be found in the appendix. (Ravichandra & Hanif, 2015) explores how Bollinger Bands, combined with the RSI generate returns in the options and futures segments of the Indian market. 

(Darmawan Et al, 2024) combines fuzzy logic with Bollinger Bands to improve decision-making in times of increased volatility, based off (Chlif Et al, 2023). The implementation of fuzzy logic with Bollinger Bands exhibited a higher level of precision compared with the MACD indicator and traditional BB, highlighting its efficacy in producing trade signals. Please see Appendix C for more information. To encapsulate the previous paragraphs, (Lento Et Al, 2007) evaluated the profitability of BB indicators, and found that after accounting for transaction costs, the traditional BB strategy struggles to outperform a simple buy and hold strategy, however, profitability improves when applied within a contrarian trading framework (Appendix D)

# Approach & Method

I use a combination of data tables, histograms, boxplots, and screenshots from the Shiny UI.

The scope of this project is the S&P500 constituents, which are pulled using the blpapi package from the Bloomberg Terminal in TPSC1.03. The stock prices are then converted into an Excel file, which is used for the remainder of the project to significantly reduce the number of API calls made, which is time-consuming. 

```{r, eval = FALSE}
# Load necessary libraries
library(Rblpapi)
library(tidyverse)

# Connect to Bloomberg
blpConnect()

# Define the full list of S&P 500 tickers in Bloomberg format
# Assuming `ticker_full` is your predefined list of tickers in "US Equity" format

# Define batch size and split `ticker_full` into batches of 100
batch_size <- 100
ticker_batches <- split(ticker_full, ceiling(seq_along(ticker_full) / batch_size))

# Initialize an empty list to store data for each batch
all_data <- list()

# Loop through each batch
for (i in seq_along(ticker_batches)) {
    batch <- ticker_batches[[i]]
    batch_data <- list()  # Temporary list to store each ticker's data within the batch
    
    # Loop through each ticker in the current batch
    for (ticker in batch) {
        # Fetch the adjusted close prices for the current ticker
        ticker_data <- bdh(
            securities = ticker,
            fields = "PX_LAST",  
            start.date = start_date,
            end.date = end_date
        )
        
        # Store the result in the batch_data list with the ticker as the name
        batch_data[[ticker]] <- ticker_data
        
        # Pause for 2 seconds before the next request
        Sys.sleep(2)
    }
    
    # Combine the batch data and add it to all_data
    all_data[[i]] <- bind_rows(batch_data, .id = "ticker")
}

# Combine all batches into a single data frame
price_data <- bind_rows(all_data)

# Optionally, reshape to have dates as rows and tickers as columns
price_data_wide <- price_data %>%
    pivot_wider(names_from = ticker, values_from = PX_LAST) %>%
    rename(Date = date)

# Display the head of the final combined data
print(head(price_data_wide))

save.image(file = "Machine_learning_noob.RData")
summary(price_data_wide)
dim(price_data_wide)

# Load required library
library(openxlsx)

# Load the data from your specified file path
load("C:/Users/jakes/Downloads/Machine_learning_noob.RData")

# Check if 'price_data_wide' is loaded successfully
if (exists("price_data_wide")) {
  # Create a new workbook
  wb <- createWorkbook()
  
  # Add a worksheet named "S&P500 Data"
  addWorksheet(wb, "S&P500 Data")
  
  # Write the entire dataframe to the Excel worksheet
  writeData(wb, "S&P500 Data", price_data_wide, startCol = 1, startRow = 1)
  
  # Save the workbook as an Excel file
  saveWorkbook(wb, "C:/Users/jakes/Downloads/SP500_Data.xlsx", overwrite = TRUE)
  
  # Confirm output
  cat("Excel file saved as 'C:/Users/jakes/Downloads/SP500_Data.xlsx'")
} else {
  cat("Error: 'price_data_wide' not found in the loaded RData file.")
}
```

The Bollinger Bands are created through a rule-based system, based on predefined thresholds for the Z-statistic. The logic BUY is made if Z is less than -2, SELL if Z is more than 2, and hold if the price is within the bands.

## Z-Statistic Formula

The Z-Statistic can be calculated using the following formula:

\[
Z = \frac{P - BB_{Mean}}{\sigma}
\]

Where:
- \( Z \): Z-Statistic

- \( P \): Current price of the stock

- \( BB_{Mean} \): The middle Bollinger Band (typically a simple moving average)

- \( \sigma \): Standard deviation of prices over the selected period.

```{r, eval = FALSE}
# Load libraries
library(openxlsx)
library(quantmod)
library(tidyverse)
library(zoo)
library(shiny)
library(DT)

# Step 1: Load and Clean Price Data
price_data <- price_data_wide  # Replace with your actual dataset

price_data <- price_data %>%
    mutate(Date = as.Date(Date)) %>%
    column_to_rownames("Date") %>%
    select(where(~ any(!is.na(.)))) %>% 
    mutate(across(everything(), as.numeric))  # Ensure all values are numeric

# Step 2: Function to Calculate Bollinger Bands and Z-Statistic
calculate_signals <- function(price_series, n = 20, k = 2) {
    if (all(is.na(price_series))) return(NULL)
    
    BB_Upper <- BB_Lower <- BB_Mean <- Z <- rep(NA, length(price_series))
    signal <- rep("HOLD", length(price_series))  # Default signal
    
    valid_indices <- which(!is.na(price_series))
    if (length(valid_indices) >= n) {
        bb <- BBands(price_series[valid_indices], n = n, sd = k)
        rolling_sd <- rollapply(price_series[valid_indices], n, sd, fill = NA, align = "right")
        
        BB_Mean[valid_indices] <- bb[, "mavg"]
        BB_Upper[valid_indices] <- bb[, "up"]
        BB_Lower[valid_indices] <- bb[, "dn"]
        Z[valid_indices] <- (price_series[valid_indices] - BB_Mean[valid_indices]) / rolling_sd
        
        signal[valid_indices][price_series[valid_indices] < BB_Lower[valid_indices] & Z[valid_indices] < -2] <- "BUY"
        signal[valid_indices][price_series[valid_indices] > BB_Upper[valid_indices] & Z[valid_indices] > 2] <- "SELL"
    }
    
    return(data.frame(
        Date = rownames(price_data),
        Price = price_series,
        BB_Upper = BB_Upper,
        BB_Lower = BB_Lower,
        BB_Mean = BB_Mean,
        Z_Statistic = Z,
        Signal = signal
    ))
}

# Step 3: Apply Function and Save to Excel
signals_list <- lapply(price_data, calculate_signals)
names(signals_list) <- colnames(price_data)

save_to_excel <- function(signals_list, file_name = "bollinger_signals.xlsx") {
    workbook <- createWorkbook()
    
    for (stock in names(signals_list)) {
        if (!is.null(signals_list[[stock]])) {
            sheet_data <- signals_list[[stock]]
            sheet_name <- str_replace_all(stock, "[^A-Za-z0-9_]", "_")
            addWorksheet(workbook, sheetName = sheet_name)
            writeData(workbook, sheet = sheet_name, sheet_data)
        }
    }
    
    saveWorkbook(workbook, file_name, overwrite = TRUE)
}

save_to_excel(signals_list, "bollinger_signals.xlsx")
print("Excel file saved successfully.")
```

# Visualisations

Once the Excel spreadsheet with adjusted closing prices for each constituent is made, I combined ggplot and plotly to allow the user to see a time-series graph of their desired stock(s)

## Shiny UI

The following image demonstrates the user interface of the Shiny application created for this project:

![Shiny UI Screenshot](ShinyUI.png)

This screenshot illustrates the functionality of the interactive dashboard, showcasing features such as sliders, plots, and real-time visualisations.

## Bollinger Bands

The bollinger signals file is first called in to the environment:

```{r, eval = FALSE}
library(readxl)
bollinger_signals <- read_excel("bollinger_signals.xlsx")
```

The Shiny UI is then simulated in the console: 
```{r, eval = FALSE}
# Step 4: Dynamic Visualization with Shiny
ui <- fluidPage(
    
    # Title
    titlePanel("Dynamic Bollinger Signals Visualization"),
    
    # Sidebar layout
    sidebarLayout(
        sidebarPanel(
            selectInput("stock", "Select Stock:", choices = names(signals_list)),
            dateRangeInput("date_range", "Select Date Range:", start = min(price_data$Date), end = max(price_data$Date))
        ),
        
        mainPanel(
            plotOutput("combined_plot"),
            DTOutput("signals_table")
        )
    )
)

server <- function(input, output, session) {
    
    filtered_data <- reactive({
        req(input$stock, input$date_range)
        data <- signals_list[[input$stock]]
        data <- data %>% filter(Date >= input$date_range[1] & Date <= input$date_range[2])
        return(data)
    })
    
    output$combined_plot <- renderPlot({
        req(filtered_data())
        data <- filtered_data()
        
        # Create a matplotlib-like plot
        par(mfrow = c(2, 1), mar = c(4, 4, 2, 1))
        
        # Plot price and Bollinger Bands
        plot(as.Date(data$Date), data$Price, type = "l", col = "blue", lwd = 2, xlab = "Date", ylab = "Price",
             main = paste("Bollinger Bands for", input$stock))
        lines(as.Date(data$Date), data$BB_Upper, col = "red", lwd = 1.5)
        lines(as.Date(data$Date), data$BB_Lower, col = "green", lwd = 1.5)
        lines(as.Date(data$Date), data$BB_Mean, col = "orange", lwd = 1.5)
        legend("topright", legend = c("Price", "BB Upper", "BB Lower", "BB Mean"), 
               col = c("blue", "red", "green", "orange"), lty = 1, cex = 0.8)
        
        # Plot Z-Statistic
        plot(as.Date(data$Date), data$Z_Statistic, type = "l", col = "purple", lwd = 2, xlab = "Date", ylab = "Z-Statistic",
             main = "Z-Statistic over Time")
        abline(h = c(-2, 2), col = "gray", lty = 2)
    })
    
    output$signals_table <- renderDT({
        req(filtered_data())
        datatable(filtered_data())
    })
}

shinyApp(ui, server)
```

```{r, echo=FALSE}
knitr::include_graphics("BollingerBandsShiny.png")
```




# Bibliography

Bollinger, J., 1992. Using bollinger bands. Stocks & Commodities, 10(2), pp.47-51. 

Chen, S., Zhang, B., Zhou, G., & Qin, Q. (2018). Bollinger Bands Trading Strategy Based on Wavelet Analysis. Applied Economics and Finance, 5(3), pp. 49–57. Available at: https://doi.org/10.11114/aef.v5i3.3079

Yan, K., Wang, Y. and Li, Y., 2023. Enhanced Bollinger Band Stock Quantitative Trading Strategy Based on Random Forest. Artificial Intelligence Evolution, pp.22-33.

Ravichandra, T. and Hanif, M., 2015. Bollinger bands optimal algorithmic strategyinstock trading. International Journal of Research in Finance and Marketing, 5(1), pp.1-9.

Darmawan, O. A., Heryadi, Y., Lukas, Wulandhari, L. A., & Sonata, I. (2024). The Utilization of Fuzzy Logic and Bollinger Bands to Enhance Trading Decision-Making During the Bitcoin Halving Phase. Procedia Computer Science, 245, pp. 272–281. Available at: https://www.sciencedirect.com/science/article/pii/S2452452024000272

H. Chlif, D. Kanzari and Y. Ben Said, "An Adaptive Neuro Fuzzy to Predict Cryptocurrency Based on the Crisp Method: Case of COVID-19," 2023 IEEE International Conference on Advances in Data-Driven Analytics And Intelligent Systems (ADACIS), Marrakesh, Morocco, 2023, pp. 1-6, doi: 10.1109/ADACIS59737.2023.10424284.

Lento, C., Gradojevic, N. and Wright, C.S., 2007. Investment information content in Bollinger Bands?. Applied Financial Economics Letters, 3(4), pp.263-267.

Torrence, C. and Compo, G.P., 1998. A practical guide to wavelet analysis. Bulletin of the American Meteorological society, 79(1), pp.61-78.

Manimaran, P., Panigrahi, P.K. and Parikh, J.C., 2005. Wavelet analysis and scaling properties of time series. Physical Review E—Statistical, Nonlinear, and Soft Matter Physics, 72(4), p.046120.

Jansen, M., 2012. Noise reduction by wavelet thresholding (Vol. 161). Springer Science & Business Media.

Breiman, L., 2001. Random forests. Machine learning, 45, pp.5-32.

Biau, G. and Scornet, E., 2016. A random forest guided tour. Test, 25, pp.197-227.

Lin, Y. and Jeon, Y., 2006. Random forests and adaptive nearest neighbors. Journal of the American Statistical Association, 101(474), pp.578-590.

Zadeh, L.A., 1988. Fuzzy logic. Computer, 21(4), pp.83-93.

Lauguico, S., Concepcion II, R., Alejandrino, J., Macasaet, D., Tobias, R.R., Bandala, A. and Dadios, E., 2019, November. A fuzzy logic-based stock market trading algorithm using bollinger bands. In 2019 IEEE 11th international conference on humanoid, nanotechnology, information technology, communication and control, environment, and management (HNICEM) (pp. 1-6). IEEE.

Cheung, W.M. and Kaymak, U., 2007, November. A fuzzy logic based trading system. In Proceedings of the Third European Symposium on Nature-inspired Smart Information Systems (Vol. 59, pp. 1-60).

# Appendix

## Wavelet analysis

Wavelet analysis is a mathematical tool for analysing non-stationary and noisy time-series data. Unlike Fourier Transform, which provides frequency-domain information without time localisation, wavelet analysis offers a multi-resolution approach, allowing for simultaneous examination of time and frequency components; theorised to be effective in financial markets, whereby trends and patterns occur at varying scales.

As wavelet analysis decomposes a time-series into components at multiple scales using wavelet basis functions, a wavelet function satisfies the following properties:

### Zero Mean

$$
\int_{-\infty}^{\infty} \varphi(t) dt = 0
$$

This property ensures the wavelet function has equal positive and negative values, which cancel each other out when integrated across the entire time domain. This allows wavelets to detect changes or localised features (like sharp transitions) in the data, as opposed to representing constant trends.

### Finite Energy

$$
\int_{-\infty}^{\infty} |\varphi(t)|^2 dt < \infty
$$

This property ensures the wavelet function is localised in time, meaning it is significant within a finite region, which diminishes rapidly outside that region. This is ideal for analysing signals with transient changes, which exist in financial time-series (Torrence and Compo, 1998).

Based on this theoretical foundation, wavelet decomposition simply decomposes signal $v(t)$ into low-frequency (trend) and high-frequency (detail or noise) components.

Scaling coefficients represent the smooth, low-frequency components of the signal, where $\varphi_{jk}(t)$ is the scaling function.

Wavelet coefficients represent the fine, high-frequency components capturing abrupt changes, whereby $\varphi_{jk}(t)$ is the wavelet function.

$$
c_{jk} = \langle v(t), \varphi_{jk}(t) \rangle
$$

The signal can be expressed as:

$$
v(t) = \sum_k c_{jk} \varphi_{jk}(t) + \sum_{j=j}^{\infty} \sum_k d_{jk} \varphi_{jk}(t)
$$

Hard thresholding is applied for noise reduction. Noise is effectively removed whilst retaining significant trends. This method preserves larger coefficients, whilst retaining sharp transitions in data, whereas discontinuities may be introduced at the threshold boundary. In contrast, soft thresholding shrinks coefficients towards zero by subtracting the threshold value. This helps yield smoother results and avoid discontinuities, yet this might distort the data for smaller coefficients. This area could be explored in the future to optimise data structures (Manimaran et al., 2005).

$$
T_{\text{thr}} = \sqrt{2 \log(n)} \cdot \sigma
$$

After thresholding, the filtered signal is reconstructed by combining the remaining coefficients (Jensen, 2012):

$$
v_{\text{filtered}}(t) = \sum_k c_{jk} \varphi_{jk}(t) + \sum_{j=j}^{\infty} \sum_k d_{jk} \varphi_{jk}(t)
$$

## B – Random Forest Algorithm

The advent of large datasets in modern applications necessitates learning algorithms that scale effectively with data volume while maintaining robust statistical performance. Random Forests, developed by (Breiman, 2001), are among the most successful methods for handling such data. As a supervised learning procedure, Random Forests build an ensemble of decision trees using the "divide and conquer" principle: splitting the dataset into random subsets, training individual predictors on these subsets, and aggregating their outputs.

Whilst Random Forests are empirically effective, their theoretical underpinnings are much less developed (Biau & Scornet, 2016). (Breiman, 2001) laid out the foundation for Random Forests, providing an upper bound on the generalisation error, acknowledging that tree correlation through randomisation improves ensemble performance, whilst pertaining to the limited theoretical understanding of these interactions. (Lin & Jeon, 2006) explore the connection between Random Forests and nearest neighbour predictions.

## C – Fuzzy Logic

Fuzzy logic is a form of reasoning that deals with approximate rather than fixed reasoning. Unlike binary logic (0 and 1), it operates on degrees of truth (Zadeh, 1988). In the context of Bollinger Bands, the variables derived from Bollinger Bands are converted into fuzzy inputs such as distance from the bands, and band width. Membership functions represent the degree of membership of a variable in linguistic categories (for example: “low”, “medium”, and “high”). Membership functions define how each point in the input space is mapped to a degree of membership within the fuzzy set.

These degrees are represented as values between 0 and 1, where 0 means no membership, and 1 means full membership. The key purpose of this is to handle uncertainty and imprecision in data. The most common membership function for stock market data would be the Gaussian membership function as it’s argued that this type of distribution mimics natural distributions (Lauguico, 2019), (Darmawan, 2024). The Gaussian membership function takes the form:

$$
\mu_A(x) = e^{\frac{-(x-c)^2}{2\sigma^2}}
$$

The two parameters are:

- **c** – Center  
- **σ** – Standard Deviation  

It’s important to note the parameters “low”, “medium”, “high” take the Triangular membership function.

After this, trading signals are based on the fuzzy inputs; rules are constructed using the “IF-THEN” format. For example:

> *“If the price is very close to the lower band AND the band width is wide, THEN generate a strong buy signal.”*

> *“If the price is moderately close to the upper band AND the band width is narrow, THEN generate a weak sell signal.”*

A fuzzy inference system processes the fuzzy inputs, applies the rules to determine the output. Bollinger Bands models tend to use either the Sugeno or Mamdani fuzzy inference system (Cheung and Kaymak, 2007). The Mamdani fuzzy inference system then leads to defuzzification.

## D – Contrarian Framework

In Leyman’s terms, a contrarian approach is whereby investors act opposite to prevailing market trends or sentiment. Popular market opinions lead to mispricing of assets due to herd behaviour and emotional decision-making. An example of an individual who uses the Contrarian framework would be one who buys low and sells high.

